\chapter{Content-Aware image REstoration (CARE) ... Trainable denoisers?}

Image restoration is the problem of reconstructing an image from a corrupted version of itself. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\chapter{Supervised training}

Supervised CARE networks compute
\begin{equation}
  y^* = E(x)
\end{equation}
where $x$ is a noisy (in general, distorted) image, and $y^*$ is the restored image, and the networks are trained by minimizing
\begin{equation}
  \mathcal{L}(E(x), y),
\end{equation}
where $\mathcal{L}$ represents some distortion metric (a loss
function). % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

Supervised training can be used only is we good quality GT images $y$
are
available. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Unsupervised training}

Unsupervised methods do not require high-quality, low-noise data
(ground-truth).  Such approaches try to utilize internal statistics of
the presented data to perform image
restoration % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Noise2Noise training}
% Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., et al. (2018). Noise2Noise: Learning image restoration without clean data. arXiv, Retrieved from http://arxiv.org/abs/1803.04189.

Minimizes
\begin{equation}
  {\mathcal L}(E(x_1),x_2),
\end{equation}
where $x_1$ and $x_2$ are two true-noisy instances of $x$, the clean
image. As long as pairs of images with independent noise can be
acquired, Noise2-Noise enables CARE network training even in the
absence of ground-truth. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

$x_1$ and $x_2$ should be registered, in order to avoid blured
reconstructions. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

N2N if based on the law of total expectation:
\begin{align*}
\mathbb{E}_{(X,Y)}[L(f_{\theta}(X),Y)] &= \sum_{x}\sum_{y}L(f_{\theta}(x),y) p(x,y). \\
\intertext{Factor the joint pmf:}
&= \sum_{x}\sum_{y}L(f_{\theta}(x),y) p(y|x) p(x). \\
\intertext{Group terms by $x$:}
&= \sum_{x}\left(\sum_{y}L(f_{\theta}(x),y) p(y|x)\right) p(x). \\
\intertext{The inner sum is exactly $\mathbb{E}_{Y|X=x}[L(f_{\theta}(x),Y)]$. The outer sum is then the expectation over $X$:}
&= \sum_{x} \mathbb{E}_{Y|X=x}[L(f_{\theta}(x),Y)] p(x) \\
&= \mathbb{E}_{X}[\mathbb{E}_{Y|X}[L(f_{\theta}(X),Y)]].
\end{align*}

\section{Noise2Void training}
% Krull, A., Buchholz, T.-O., & Jug, F. (2018). Noise2Void—Learning denoising from single noisy images. arXiv. Retrieved from http://arxiv.org/abs/1811.10980.

This training regime requires to derive both, the input and the target
data, from single noisy images. Motivated by the non-existing target
image, this training approach is called Noise2Void. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub


The fundamental idea of Noise2Void is to take out a single pixel in
the center of a receptive field, thereby creating a “blind-spot.” We
can then use this removed pixel value as target for learning a network
that will predict the value hidden in the blind-spot. Note, this
target is not the ground-truth pixel value, since it is itself only
taken from the noisy input image. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

Two conditions must be fulfilled for Noise2Void training to
succeed. The data must be statistically interdependent, meaning that
knowing the surrounding of one pixel allows an observer to predict
that pixel’s value. Additionally, the noise in the image needs to be
pixel-wise independent (given the signal). % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Noise2Self training}

%{{{

Cryo-CARE \cite{buchholz2019cryo} uses a denoising U-Net for
tomographic reconstruction according to the Noise2Noise (N2N)
\cite{lehtinen2018noise2noise} training paradigm
\url{https://github.com/juglab/cryoCARE_pip}.

CARE (Content-Aware image REstoration) methods leverage available
knowledge about the data at hand ought to yield superior restoration
results \cite{weigert2018content}.

% Spatiotemporal-Aware Self-Supervised Fluorescence Microscopy Image Denoising
\begin{quote}
  As stated by noise2noise, the denoising model trained with L2 loss tends to outputs the mean of multiple independent noisy observations per secene, to restore the clean target.
\end{quote}

N2N is a ``supervised'' learning method for denoising where an
autoencoder neural network with skip connections (a U-Net) is trained
on pairs of noisy images. However, unlike clasical supervised
denoising deep-learning based models, that usually implement
\cite{lehtinen2018noise2noise}
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j\big)
\end{equation}
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\{\hat{\mathbf X}\}_j), \{{\mathbf X}\}_j\big)
\end{equation}

where $\{(\hat{\mathbf{X}}, \mathbf{X})\}_j\}$ is the training
dataset, and $L$ is a given lost function such as the MSE, N2N solve

where $\{(\hat{\mathbf X}_j^{(1)}, {\mathbf X}_j)\}_{j=1}^M$ is the training
dataset, and $L$ is a given lost function such as the MSE, N2N solves
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j^{(2)}\big).
\end{equation}
In other words, given two noisy versions
$\{\hat{\mathbf Y}^{(1)}, \hat{\mathbf Y}^{(2)}\}$ of the same (clean)
volume ${\mathbf Y}$, N2N learns to infeer a denoised volume
\begin{equation}
  \tilde{\mathbf Y}=\frac{1}{2}\big(f_\theta(\hat{\mathbf Y}^{(1)})+f_\theta(\hat{\mathbf Y}^{(2)})\big)\approx{\mathbf Y}.
\end{equation}
Obviously, better approximations to ${\mathbf Y}$ will be obtained
having more noisy instances, after averaing all the denoised volumes.

%}}}
