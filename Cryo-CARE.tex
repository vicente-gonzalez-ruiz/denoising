\chapter{Content-Aware image REstoration (CARE) ... Trainable denoisers?}

Image restoration is the problem of reconstructing an image from a corrupted version of itself. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\chapter{Supervised training}

Supervised CARE networks compute
\begin{equation}
  y^* = E(x)
\end{equation}
where $x$ is a noisy (in general, distorted) image, and $y^*$ is the restored image, and the networks are trained by minimizing
\begin{equation}
  \mathcal{L}(E(x), y),
\end{equation}
where $\mathcal{L}$ represents some distortion metric (a loss
function). % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

Supervised training can be used only is we good quality GT images $y$
are
available. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Unsupervised training}

Unsupervised methods do not require high-quality, low-noise data
(ground-truth).  Such approaches try to utilize internal statistics of
the presented data to perform image
restoration % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Noise2Noise training}
% Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., et al. (2018). Noise2Noise: Learning image restoration without clean data. arXiv, Retrieved from http://arxiv.org/abs/1803.04189.


Minimizes
\begin{equation}
  {\mathcal L}(E(x_1),x_2),
\end{equation}
where $x_1$ and $x_2$ are two true-noisy instances of $x$, the clean
image. As long as pairs of images with independent noise can be
acquired, Noise2-Noise enables CARE network training even in the
absence of ground-truth. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

$x_1$ and $x_2$ should be registered, in order to avoid blured
reconstructions. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

N2N if based on the law of total expectation:
\begin{align*}
\mathbb{E}_{(X,Y)}[L(f_{\theta}(X),Y)] &= \sum_{x}\sum_{y}L(f_{\theta}(x),y) p(x,y). \\
\intertext{Factor the joint pmf:}
&= \sum_{x}\sum_{y}L(f_{\theta}(x),y) p(y|x) p(x). \\
\intertext{Group terms by $x$:}
&= \sum_{x}\left(\sum_{y}L(f_{\theta}(x),y) p(y|x)\right) p(x). \\
\intertext{The inner sum is exactly $\mathbb{E}_{Y|X=x}[L(f_{\theta}(x),Y)]$. The outer sum is then the expectation over $X$:}
&= \sum_{x} \mathbb{E}_{Y|X=x}[L(f_{\theta}(x),Y)] p(x) \\
                                       &= \mathbb{E}_{X}[\mathbb{E}_{Y|X}[L(f_{\theta}(X),Y)]].
                                         \label{eq:5}
\end{align*}

The usual process of training regressors by
Eq.~\ref{eq:supervised_learning} over a finite number of input-target
pairs (xi, yi) hides a subtle point: instead of the 1:1 mapping
between inputs and tar- gets (falsely) implied by that process, in
reality the mapping is multiple-valued. For example, in a superresolution task
(Ledig et al., 2017) over all natural images, a low-resolution
image x can be explained by many different high-resolution
images y, as knowledge about the exact positions and orientations of the edges and texture is lost in decimation. In
other words, $p(y|x)$ is the highly complex distribution of
natural images consistent with the low-resolution $x$. Train-
ing a neural network regressor using training pairs of low-
and high-resolution images using the L2 loss, the network
learns to output the average of all plausible explanations
(e.g., edges shifted by different amounts), which results in
spatial blurriness for the network’s predictions.

Our observation is that for certain problems this tendency
has an unexpected benefit. A trivial, and, at first sight, use-
less, property of L2 minimization is that on expectation, the
estimate remains unchanged if we replace the targets with
random numbers whose expectations match the targets. Consequently, the
optimal network parameters $\Theta$ of Equation (\ref{eq:5}) also remain
unchanged, if input-conditioned target distributions $p(y|x)$
are replaced with arbitrary distributions that have the same
conditional expected values. This implies that we can, in
principle, corrupt the training targets of a neural network
with zero-mean noise without changing what the network
learns. Combining this with the corrupted inputs from Equa-
tion \ref{eq:supervised_learning}, we are left with the empirical risk minimization task
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_i L \big(f_\theta(\hat{\mathbf x}_i), \hat{\mathbf y}_i\big),
\end{equation}
where both the inputs and the targets are now drawn from a corrupted
distribution (not necessarily the same), condi- tioned on the
underlying, unobserved clean target $y_i$ such that
$E{\hat{y}_i|\hat{x}_i} = y_i$. Given infinite data, the solution is
the same as that of \ref{eq:supervised_learning} . For finite data,
the variance is the average variance of the corruptions in the
targets, divided by the number of training samples (see
appendix). Inter- estingly, none of the above relies on a likelihood
model of the corruption, nor a density model (prior) for the under-
lying clean image manifold. That is, we do not need an explicit
p(noisy|clean) or p(clean), as long as we have data distributed
according to them.

\section{Noise2Void training}
% Krull, A., Buchholz, T.-O., & Jug, F. (2018). Noise2Void—Learning denoising from single noisy images. arXiv. Retrieved from http://arxiv.org/abs/1811.10980.

This training regime requires to derive both, the input and the target
data, from single noisy images. Motivated by the non-existing target
image, this training approach is called Noise2Void. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub


The fundamental idea of Noise2Void is to take out a single pixel in
the center of a receptive field, thereby creating a “blind-spot.” We
can then use this removed pixel value as target for learning a network
that will predict the value hidden in the blind-spot. Note, this
target is not the ground-truth pixel value, since it is itself only
taken from the noisy input image. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

Two conditions must be fulfilled for Noise2Void training to
succeed. The data must be statistically interdependent, meaning that
knowing the surrounding of one pixel allows an observer to predict
that pixel’s value. Additionally, the noise in the image needs to be
pixel-wise independent (given the signal). % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Noise2Self training}

%{{{

Cryo-CARE \cite{buchholz2019cryo} uses a denoising U-Net for
tomographic reconstruction according to the Noise2Noise (N2N)
\cite{lehtinen2018noise2noise} training paradigm
\url{https://github.com/juglab/cryoCARE_pip}.

CARE (Content-Aware image REstoration) methods leverage available
knowledge about the data at hand ought to yield superior restoration
results \cite{weigert2018content}.

% Spatiotemporal-Aware Self-Supervised Fluorescence Microscopy Image Denoising
\begin{quote}
  As stated by noise2noise, the denoising model trained with L2 loss tends to outputs the mean of multiple independent noisy observations per secene, to restore the clean target.
\end{quote}

N2N is a ``supervised'' learning method for denoising where an
autoencoder neural network with skip connections (a U-Net) is trained
on pairs of noisy images. However, unlike clasical supervised
denoising deep-learning based models, that usually implement
\cite{lehtinen2018noise2noise}
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j\big)
\end{equation}
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\{\hat{\mathbf X}\}_j), \{{\mathbf X}\}_j\big)
\end{equation}

where $\{(\hat{\mathbf{X}}, \mathbf{X})\}_j\}$ is the training
dataset, and $L$ is a given lost function such as the MSE, N2N solve

where $\{(\hat{\mathbf X}_j^{(1)}, {\mathbf X}_j)\}_{j=1}^M$ is the training
dataset, and $L$ is a given lost function such as the MSE, N2N solves
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j^{(2)}\big).
\end{equation}
In other words, given two noisy versions
$\{\hat{\mathbf Y}^{(1)}, \hat{\mathbf Y}^{(2)}\}$ of the same (clean)
volume ${\mathbf Y}$, N2N learns to infeer a denoised volume
\begin{equation}
  \tilde{\mathbf Y}=\frac{1}{2}\big(f_\theta(\hat{\mathbf Y}^{(1)})+f_\theta(\hat{\mathbf Y}^{(2)})\big)\approx{\mathbf Y}.
\end{equation}
Obviously, better approximations to ${\mathbf Y}$ will be obtained
having more noisy instances, after averaing all the denoised volumes.

%}}}
