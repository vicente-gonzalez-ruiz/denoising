\chapter{Denoising auto-encoders (DAEs)}

An autoencoder is a type of artificial neural network trained to
generate an output related in some way with the input. For example, in
image compression, the input and the output are identical, and the
network tries to reproduce such behaviour. In image denoising, the
network is trained to remove the noise, i.e., to generate a clean
version of the input.

\section{Data-driven VS zero-shot methods}
Data-driven approaches rely on large datasets with high-quality
labels. Zero-shot methods avoid this limitation but remain effective
only for independent and identically distributed (i.i.d.) noise. % Median2Median: Zero-shot Suppression of Structured Noise in Images, J Wang, G Wang - arXiv preprint arXiv:2510.01666, 2025

In contrast to supervised methods, zero-shot denoising offers a
compelling alternative by training a network directly on a single
noisy input, without requiring any external dataset. % https://arxiv.org/pdf/2510.01666

Zero-shot denoising methods learn to denoise directly from a single
noisy image without requiring any external dataset, making them
particularly advantageous in data-limited settings. Unlike supervised (DnCNN and FFDNet)
and self-supervised methods (N2V and Ne2Ne), zero-shot methods are not sensitive to
any distribution shift of test data, allowing superior
generalizability across noise types and image domains, since training
and inference occur on the same image.

\section{Supervised VS unsupervised methods}
Unsupervised are also called blind methods. % https://arxiv.org/pdf/2510.01666

Supervised denoising methods use paired datasets to train neural
networks to recover clean images from noisy inputs (N2C).  Early methods
such as DnCNN [4] and FFDNet [11] use clean–noisy image pairs to
learn a denoising mapping. % https://arxiv.org/pdf/2510.01666

[4] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual
learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142–3155, 2017.

[11] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image
denoising. IEEE Transactions on Image Processing, 27(9):4608–4622, 2018.

N2C training offers a strong performance benchmark and makes no
assumption about the underlying noise characteristics, whether the
noise is structured or unstructured, following any statistical
distribution. In the supervised learning mode, the network is guided
by training data to suppress whatever noise in the dataset. However,
acquiring clean–noisy image pairs is often labor-intensive, expensive,
and even infeasible in practice, particularly in medical imaging
tasks.  % https://arxiv.org/pdf/2510.01666

\section{Compressing AEs VS denoising AEs (U-nets)}

Pure AEs, also called compressing AE, consists of two main parts:
\begin{enumerate}
\item The \textbf{encoder}, which compresses the
  input data into a lower-dimensional representation, often called a
  ``bottleneck'' or ``latent space''. This part is also called the
  contracting path of the network.
\item The \textbf{decoder} (), that takes the compressed representation
  and tries to reconstruct the original data. This is the expansive path.
\end{enumerate}

U-nets are similar, but:
\begin{enumerate}
\item Whith the idea to avoid the loss of information, the number of
  channels increases along the contracting path.
\item There exist \textbf{skip connections} between the corresponding
  layers of the contracting and the expansive paths, which must be
  symmetric.
\end{enumerate}

In both, AEs and U-nets, the encoder analyzes the content of the input
image, and the decoder tries to generate the output image with the
information that the encoder has learned. However, in U-nets the skip
connections that provide access to the decoder to the feature maps
used by the encoder, allows the decoder to keep the details of the
image that in a compressing AE are lost.

\section{Noise2Clean (N2C) training}
N2C methods train neural networks to learn end-to-end mappings
from noisy observations to clean images. % https://arxiv.org/pdf/2510.01666

Noise2Noise (N2N) [6] introduced a new training framework that learns
from pairs of noisy observations of the same underlying image, under
the assumption that the noise is zero-mean and i.i.d. This relaxation
removes the need for clean ground truth while achieving performance
comparable to N2C. % https://arxiv.org/pdf/2510.01666

[6] Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila.
Noise2noise: Learning image restoration without clean data. arXiv preprint arXiv:1803.04189, 2018.

However, the requirement of obtaining two acquisitions with
i.i.d. noise remains a serious hurdle, especially when repeated
imaging is costly, risky, or unstable due to misregistration of
subsequent images. % https://arxiv.org/pdf/2510.01666

\section{Noise2Noise (N2N) training}
N2N performs denoising using pairs of noisy observations of the same
clean image, each corrupted by independent and identically distributed
(i.d.d.) zero-mean noise. % https://arxiv.org/pdf/2510.01666

\section{Noise2Void (N2V) and Noise2Self (N2S)}
Noise2Void [7] and Noise2Self [8] adopt blind-spot networks that
exclude the central pixel during training, so the model learns to
infer the underlying signal from contextual information rather than
fitting the noise. % https://arxiv.org/pdf/2510.01666

[7] Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images.
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2129–2137, 2019.

[8] Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. In International conference on
machine learning, pages 524–533. PMLR, 2019.

Self-supervised denoising methods remove the need for paired
images. Noise2Void [ 7] masks target pixels and trains the network to
predict them using only the surrounding context. % https://arxiv.org/pdf/2510.01666

\section{Neighbor2Neighbor (Ne2Ne) [9] and Noise2Fast (N2F) [10]}
Use checkerboard downsampling to efficiently construct noisy pairs.

Neighbor2Neighbor [9] creates pseudo pairs from adjacent pixels
within the same image. Despite these innovations, self-supervised
denoising still depends on access to large datasets of noisy
images. These data-demanding methods, including both supervised and
self-supervised approaches, often fail to generalize well. That is,
they may degrade in performance when test data deviates from training
data distribution-wise. % https://arxiv.org/pdf/2510.01666

[9] Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor: Self-supervised
denoising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 14781–14790, 2021.

[10] Jason Lequyer, Reuben Philip, Amit Sharma, Wen-Hsin Hsu, and Laurence Pelletier. A fast blind zero-shot
denoiser. Nature Machine Intelligence, 4(11):953–963, 2022.

\section{Median2Median (M2M)}

\chapter{Content-Aware image REstoration (CARE) ... Trainable denoisers?}



\chapter{Supervised training}

Supervised CARE networks compute
\begin{equation}
  y^* = E(x)
\end{equation}
where $x$ is a noisy (in general, distorted) image, and $y^*$ is the restored image, and the networks are trained by minimizing
\begin{equation}
  \mathcal{L}(E(x), y),
\end{equation}
where $\mathcal{L}$ represents some distortion metric (a loss
function). % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

Supervised training can be used only is we good quality GT images $y$
are
available. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Unsupervised training}

Unsupervised methods do not require high-quality, low-noise data
(ground-truth).  Such approaches try to utilize internal statistics of
the presented data to perform image
restoration % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Noise2Noise training}
% Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., et al. (2018). Noise2Noise: Learning image restoration without clean data. arXiv, Retrieved from http://arxiv.org/abs/1803.04189.


Minimizes
\begin{equation}
  {\mathcal L}(E(x_1),x_2),
\end{equation}
where $x_1$ and $x_2$ are two true-noisy instances of $x$, the clean
image. As long as pairs of images with independent noise can be
acquired, Noise2-Noise enables CARE network training even in the
absence of ground-truth. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

$x_1$ and $x_2$ should be registered, in order to avoid blured
reconstructions. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

N2N if based on the law of total expectation:
\begin{align*}
\mathbb{E}_{(X,Y)}[L(f_{\theta}(X),Y)] &= \sum_{x}\sum_{y}L(f_{\theta}(x),y) p(x,y). \\
\intertext{Factor the joint pmf:}
&= \sum_{x}\sum_{y}L(f_{\theta}(x),y) p(y|x) p(x). \\
\intertext{Group terms by $x$:}
&= \sum_{x}\left(\sum_{y}L(f_{\theta}(x),y) p(y|x)\right) p(x). \\
\intertext{The inner sum is exactly $\mathbb{E}_{Y|X=x}[L(f_{\theta}(x),Y)]$. The outer sum is then the expectation over $X$:}
&= \sum_{x} \mathbb{E}_{Y|X=x}[L(f_{\theta}(x),Y)] p(x) \\
                                       &= \mathbb{E}_{X}[\mathbb{E}_{Y|X}[L(f_{\theta}(X),Y)]].
                                         \label{eq:5}
\end{align*}

The usual process of training regressors by
Eq.~\ref{eq:supervised_learning} over a finite number of input-target
pairs (xi, yi) hides a subtle point: instead of the 1:1 mapping
between inputs and tar- gets (falsely) implied by that process, in
reality the mapping is multiple-valued. For example, in a superresolution task
(Ledig et al., 2017) over all natural images, a low-resolution
image x can be explained by many different high-resolution
images y, as knowledge about the exact positions and orientations of the edges and texture is lost in decimation. In
other words, $p(y|x)$ is the highly complex distribution of
natural images consistent with the low-resolution $x$. Train-
ing a neural network regressor using training pairs of low-
and high-resolution images using the L2 loss, the network
learns to output the average of all plausible explanations
(e.g., edges shifted by different amounts), which results in
spatial blurriness for the network’s predictions.

Our observation is that for certain problems this tendency
has an unexpected benefit. A trivial, and, at first sight, use-
less, property of L2 minimization is that on expectation, the
estimate remains unchanged if we replace the targets with
random numbers whose expectations match the targets. Consequently, the
optimal network parameters $\Theta$ of Equation (\ref{eq:5}) also remain
unchanged, if input-conditioned target distributions $p(y|x)$
are replaced with arbitrary distributions that have the same
conditional expected values. This implies that we can, in
principle, corrupt the training targets of a neural network
with zero-mean noise without changing what the network
learns. Combining this with the corrupted inputs from Equa-
tion \ref{eq:supervised_learning}, we are left with the empirical risk minimization task
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_i L \big(f_\theta(\hat{\mathbf x}_i), \hat{\mathbf y}_i\big),
\end{equation}
where both the inputs and the targets are now drawn from a corrupted
distribution (not necessarily the same), condi- tioned on the
underlying, unobserved clean target $y_i$ such that
$E{\hat{y}_i|\hat{x}_i} = y_i$. Given infinite data, the solution is
the same as that of \ref{eq:supervised_learning} . For finite data,
the variance is the average variance of the corruptions in the
targets, divided by the number of training samples (see
appendix). Inter- estingly, none of the above relies on a likelihood
model of the corruption, nor a density model (prior) for the under-
lying clean image manifold. That is, we do not need an explicit
p(noisy|clean) or p(clean), as long as we have data distributed
according to them.

\section{Noise2Void training}
% Krull, A., Buchholz, T.-O., & Jug, F. (2018). Noise2Void—Learning denoising from single noisy images. arXiv. Retrieved from http://arxiv.org/abs/1811.10980.

This training regime requires to derive both, the input and the target
data, from single noisy images. Motivated by the non-existing target
image, this training approach is called Noise2Void. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub


The fundamental idea of Noise2Void is to take out a single pixel in
the center of a receptive field, thereby creating a “blind-spot.” We
can then use this removed pixel value as target for learning a network
that will predict the value hidden in the blind-spot. Note, this
target is not the ground-truth pixel value, since it is itself only
taken from the noisy input image. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

Two conditions must be fulfilled for Noise2Void training to
succeed. The data must be statistically interdependent, meaning that
knowing the surrounding of one pixel allows an observer to predict
that pixel’s value. Additionally, the noise in the image needs to be
pixel-wise independent (given the signal). % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Noise2Self training}

%{{{

Cryo-CARE \cite{buchholz2019cryo} uses a denoising U-Net for
tomographic reconstruction according to the Noise2Noise (N2N)
\cite{lehtinen2018noise2noise} training paradigm
\url{https://github.com/juglab/cryoCARE_pip}.

CARE (Content-Aware image REstoration) methods leverage available
knowledge about the data at hand ought to yield superior restoration
results \cite{weigert2018content}.

% Spatiotemporal-Aware Self-Supervised Fluorescence Microscopy Image Denoising
\begin{quote}
  As stated by noise2noise, the denoising model trained with L2 loss tends to outputs the mean of multiple independent noisy observations per secene, to restore the clean target.
\end{quote}

N2N is a ``supervised'' learning method for denoising where an
autoencoder neural network with skip connections (a U-Net) is trained
on pairs of noisy images. However, unlike clasical supervised
denoising deep-learning based models, that usually implement
\cite{lehtinen2018noise2noise}
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j\big)
\end{equation}
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\{\hat{\mathbf X}\}_j), \{{\mathbf X}\}_j\big)
\end{equation}

where $\{(\hat{\mathbf{X}}, \mathbf{X})\}_j\}$ is the training
dataset, and $L$ is a given lost function such as the MSE, N2N solve

where $\{(\hat{\mathbf X}_j^{(1)}, {\mathbf X}_j)\}_{j=1}^M$ is the training
dataset, and $L$ is a given lost function such as the MSE, N2N solves
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j^{(2)}\big).
\end{equation}
In other words, given two noisy versions
$\{\hat{\mathbf Y}^{(1)}, \hat{\mathbf Y}^{(2)}\}$ of the same (clean)
volume ${\mathbf Y}$, N2N learns to infeer a denoised volume
\begin{equation}
  \tilde{\mathbf Y}=\frac{1}{2}\big(f_\theta(\hat{\mathbf Y}^{(1)})+f_\theta(\hat{\mathbf Y}^{(2)})\big)\approx{\mathbf Y}.
\end{equation}
Obviously, better approximations to ${\mathbf Y}$ will be obtained
having more noisy instances, after averaing all the denoised volumes.

%}}}
