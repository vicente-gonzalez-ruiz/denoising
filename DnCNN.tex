\chapter{DnCNN}

@article{zhang2017beyond,
  title={Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising},
  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
  journal={IEEE transactions on image processing},
  volume={26},
  number={7},
  pages={3142--3155},
  year={2017},
  publisher={IEEE}
}


Able to handle
Gaussian denoising with unknown noise level (i.e., blind Gaussian
denoising)

Rather than directly outputing the denoised
image ˆx, the proposed DnCNN is designed to predict the
residual image ˆv, i.e., the difference between the noisy obser-
vation and the latent clean image. In other words, the proposed
DnCNN implicitly removes the latent clean image with the
operations in the hidden layers.

While this paper aims to design a more effective Gaussian
denoiser, we observe that when v is the difference between the
ground truth high resolution image and the bicubic upsampling
of the low resolution image, the image degradation model for
Guassian denoising can be converted to a single image super-
resolution (SISR) problem; analogously, the JPEG image
deblocking problem can be modeled by the same image
degradation model by taking v as the difference between
the original image and the compressed image. In this sense,
SISR and JPEG image deblocking can be treated as two special
cases of a “general” image denoising problem, though in SISR
and JPEG deblocking the noise v is much different from
AWGN. It is natural to ask whether it is possible to train
a single CNN model to handle such general image denoising
problem? By analyzing the connection between DnCNN and
TNRD [19], we propose to extend DnCNN for handling
several general image denoising tasks, including Gaussian
denoising, SISR and JPEG image deblocking.

The input of our DnCNN is a noisy observation
y = x + v

 For DnCNN, we adopt
the residual learning formulation to train a residual mapping
R(y) ≈ v, and then we have x = y − R(y).

Eq (1) is the loss function

