\chapter{Statistics for discrete data}

Random variables

A digital signal is the result of discretizing in amplitude a discrete
signal. Therefore, a sample of a digital signal can be represented
using a finite number of bits.

By physical requirements, digital signals are finite in length, and therefore
$\text{Sup}(\mathbf{x})$ is a finite set.

\section{Discrete signals and random variables}
%{{{

Mathematically, we can model discrete signals as \glspl{SRV}. A
\gls{SRV} can be described as a stochastic process whose statistical
properties do not change with time or space. This means, basically,
that the probability distribution of a signal at any given set of
time/space points is the same as the distribution at those same
samples shifted by any constant amount in time or space.


A digital signal is the results of discretizing a discrete signal in
amplitude. By physical reasons, a digital signal is finite in length
(the number of signal samples is a known number) and in amplitude
(each sample requires a finite number of bits to be
represented). Therefore, digital signals are discrete and finite.

A random variable is a mathematical formalization of a quantity or
object which depends on random events. When more than one value (a
vector) is generated in one of these events, we can establish a
similar connection between multicomponent signals and random
vectors. Statistically, random variables can be described throught the
mean and the variance and the corresponding distribution. In the case
of random vectors, a mean for each component, a covariance matrix, and
a multivariate distribution are required.

In this document, monocomponent (discrete) signals (and therefore
random variables) will be denoted with lower-case bold-faced symbols,
such as $\mathbf{x}$. Multicomponent (discrete) signals (random
vectors) will be denoted as $\overrightarrow{\mathbf{x}}$.

%}}}

\section{Energy of a signal}
\label{sec:energy_signal}
%{{{

By definition, the energy $E()$ of a digital signal $\mathbf{x}$ of
length $N$ is
\begin{equation}
  E(\mathbf{x}) = \sum_{i}|\mathbf{x}_i|^2.
\end{equation}

Notice that, by definition, the energy of a digital signal is finite,
but grows with $N$.

%}}}

\section{Power of a signal}
\label{sec:power_signal}
%{{{

By definition, the power $P()$ of a digital signal
$\mathbf{x}$ of length $N$ is
\begin{equation}
  P(\mathbf{x}) = \frac{1}{N}E(\mathbf{x}),
\end{equation}
i.e., its average energy.

Notice that when $N$ is big, the use of the power of a signal instead
of the energy can help to avoid arithmetic overflow.

%}}}

\section{Fourier transform}
\label{sec:Fourier_transform}
%{{{

The Fourier transform, usually denoted by $\mathcal{F}()$, of a
continuous function $f(x)$ is
\begin{equation}
  \mathcal{F}(f) = F(w) = \int_{-\infty}^{\infty}f(x)e^{-jwx}dx
  \label{eq:FT}
\end{equation}
and its inverse, denoted by $\mathcal{F}^{-1}$ is
\begin{equation}
  f(x) = \mathcal{F}^{-1}(F(w)) = \int_{-\infty}^{\infty}F(w)e^{jxw}dw,
\end{equation}
where $w=2\pi f$ denotes angular frequency, $j=\sqrt{-1}$, and
$f$ is frequency. We will denote the inverse Fourier transform of
$\cdot$ by $\mathcal{F}^{-1}(\cdot)$.

Notice that, the Fourier transform of a signal $f$ is a complex signal
$F$, even if $f$ is real.

%}}}

\section{Fourier transform of a discrete signal}
\label{sec:FTDF}
%{{{

If the signal $x$ is discrete, its Fourier transform, also
known as the \gls{DTFT}, is
\begin{equation}
  \mathcal{F}(x[n]) = X(e^{jw}) = \sum_{n=-\infty}^{\infty}x[n]e^{-jwn}
\end{equation}
which is a continuous function of $w$, and its inverse is
\begin{equation}
  x[n] = \mathcal{F}^{-1}(X(e^{jw})) = \frac{1}{2\pi}\int_{-\pi}^{\pi}X(e^{jw})e^{jwn}dw.
\end{equation}
The \acrshort{FTDF} is periodic with period $2\pi$.

%}}}

\section{Discrete Fourier Transform (DFT)}
\label{sec:DFT}
%{{{

The \gls{DFT} of a digital\footnote{The \gls{DFT} is defined over
  digital (discrete and finite) signals.} signal $\mathbf{x}$ of
lenght $N$ is defined as
\begin{equation}
  \mathcal{F}(\mathbf{x}_n) = \mathbf{X}_f=\sum_{n=0}^{N-1}\mathbf{x}_ne^{-2\pi jf\frac{n}{N}},\quad f=0,1,\cdots,N-1
  \label{eq:DFT}
\end{equation}
where $f$ denotes (discrete) frequency bins. The inverse \gls{DFT} is
\begin{equation}
  \mathbf{x}_n=\frac{1}{N}\sum_{f=0}^{N-1}\mathbf{X}_fe^{2\pi jn\frac{f}{N}}, \quad  n=0,1,\cdots,N-1.
\end{equation}

Notice that the \gls{DFT} can be interpreted as sampling the
\gls{FTDF} at $N$ evenly spaced points over $[-\pi, \pi)$. So, the
\gls{DFT} is a discrete approximation of the \gls{FTDF} for
finite-length signals.

%}}}

\section{Parseval's theorem}
\label{sec:parseval}
%{{{

The Parseval's theorem for digital signals states that, except for a
scale factor, the total energy of a signal in the signal domain (time,
for example) is equal to the total energy of its representation in the
frequency domain. Concretely, if $N$ is the length of the signal
$\mathbf{s}$ and $\mathbf{S}$ is its \gls{DFT}, the Parseval’s theorem
states that
\begin{equation}
  E(\mathbf{s}) = \frac{1}{N}E(\mathbf{S}),
\end{equation}
where $E(\cdot)$ represents the energy operator (see
Section~\ref{sec:energy_signal}).

%}}}

\section{Fourier spectrum}
\label{sec:Fourier_spectrum}
%{{{

By definition, the Fourier
(or frequency) spectrum of $\mathbf{s}$ is the magnitude of
$\mathbf{S}$, that is
\begin{equation}
  |\mathbf{S}| = \sqrt{(\text{Re}(\mathbf{S}))^2+(\text{Im}(\mathbf{S}))^2}.
\end{equation}
As a complex function, we can also find the \emph{phase angle}, \emph{argument} or
\emph{phase spectrum} using
\begin{equation}
  \arg({\mathbf{S}}) = \text{arctan}\left(\frac{\text{Re}(\mathbf{S})}{\text{Im}(\mathbf{S})}\right),
\end{equation}
where $\text{arctan}()$ must be computed using a four-quadrant
arctangent function.

%}}}

\section{Power spectrum (PS)}
\label{sec:power_spectrum}
%{{{

The power spectrum $\text{PS}()$ of a digital signal $\mathbf{x}$
shows how much power a signal has at different frequencies. Therefore,
if $\mathbf{X}$ is the DFT of $\mathbf{x}$ (with $N$ samples),
\begin{equation}
  \text{PS}(\mathbf{x}) = \frac{1}{N}|X|^2
\end{equation}
is the power spectrum of $\mathbf{x}$.

Notice that, for real-valued digital signals, the spectrum is
symmetric about $N/2$ (Nyquist frequency).

%}}}

\section{Convolution}
\label{sec:convolution}
%{{{

Convolution, typycally represented with the symbol $\ast$, is a
mathematical operation that combines two signals to produce a third
signal. It is often used to determine how the shape of one signal
modifies another, and in the context of digital signals convolution
models the fintering operation (one signal is the impulse response of
the filter and the other is the signal to filter).

For two digital signals $\mathbf{x}$ (the signal to filter) and
$\mathbf{h}$ (the filter coefficients), the convolution is defined as
\begin{equation}
(\mathbf{x}\ast\mathbf{h})_{n}=\sum_{k\in\text{Sup}(h)}\mathbf{x}_n-\mathbf{h}_{n-k}.
\label{eq:convolution}
\end{equation}

%}}}

\section{The convolution theorem}
\label{sec:convolution_theorem}
The convolution theorem states that the convolution of two discrete signals in
the signal domain is equivalent to the multiplication of their Fourier
transforms in the frequency domain:
\begin{equation}
  \mathbf{x}\ast\mathbf{h} = \text{inverse~DTFT}(\mathbf{X}(e^{jw}\mathbf{H}(e^{jw}) 
\end{equation}

\section{Cross-correlation (CC)}
\label{sec:cross-correlation}
%{{{

\gls{CC}, usually represented by the symbol $\circledast$, measures
the similarity between two signals $\mathbf{x}$ and $\mathbf{y}$ of
one relative to the other, as a function of a time lag
$l\in\mathbb{N}$. In the case of discrete-time signals, the \gls{CC}
is defined as
\begin{equation}
  {\text{CC}(\mathbf{x},\mathbf{y})}(l)=\sum_n{\mathbf{x}}_n \mathbf{y}^*_{n-l},
\end{equation}
where $\mathbf{y}^* _{n-l}$ is the complex conjugate\footnote{If the
  signal is real-valued, then $\mathbf{y}^*_{n-l}=\mathbf{y}_{n-l}$.}
of the signal delayed by $l$ samples, and therefore
${\text{CC}(\mathbf{x},\mathbf{y})}(l)$ can take as many different
values as possible values $l$ can take (it is a function of $l$).
When the range of $l$ is undefined, we will suppose that $l$ ranges in
$\text{Su}(\mathbf{\mathbf{y}})$.

The \gls{CC} can be normalized (\gls{NCC}) using
\begin{equation}
  \text{NCC}(\mathbf{x},\mathbf{y})=\frac{{r(\mathbf{x},\mathbf{y})}}{\sqrt{\sum_n \mathbf{x}_n^2 \sum_n \mathbf{y}_n^2}}.
\end{equation}

%}}}

\section{Cross-Power Spectral Density (CPSD)}
\label{sec:CPSD}
%{{{

The \gls{CPSD}, also known as the cross-spectrum, analyzes the
relationship between two different signals in the frequency domain. In
other words, the \gls{CPSD} quantifies the degree to which two signals
are correlated or ``statistically connected'' at specific
frequencies. A high \gls{CPSD} value at a particular frequency indicates a
strong correlation between the two signals at that frequency.

The \gls{CPSD} between two signals $\mathbf{x}$ and $\mathbf{y}$ is
the Fourier transform of the \gls{CC} between these two
signals:
\begin{equation}
  \text{CPSD}(\mathbf{x},\mathbf{y})=\mathcal{F}({\text{CC}(\mathbf{x},\mathbf{y})}).
\end{equation}

Alternatively,
\begin{equation}
  \text{CPSD}(\mathbf{x},\mathbf{y})_f=E(\mathbf{X}_f\mathbf{Y}_f^*).
\end{equation}
In other words, the \gls{CPSD} for the frequency bin of index $f$ is
the expectation of the product of the Fourier coefficients
$\mathbf{X}_f$ and $\mathbf{Y}_f^*$, for at least two (or more)
segments (or instances) of the signals.

%}}}

\section{Autocorrelation}
\label{sec:autocorrelation}
%{{{

The \gls{AC} of a digital signal is another digital signal that
measures the similarity between the signal and a time-delayed version
of itself as a function of a time lag (see
Section~\ref{sec:cross-correlation}). Essentially, the \gls{AC}
quantifies the degree to which a signal is correlated with its past or
future values. Mathematically, for a discrete-time\footnote{And
  therefore, for a digital signal.} signal $\mathbf{x}$ of infinite
length, the \gls{AC} signal at a lag $l$ is defined as:
\begin{equation}
  \text{AC}(\mathbf{x})(l)=\text{CC}(\mathbf{x},\mathbf{x})(l)=\sum_n{\mathbf{x}}_n \mathbf{x}^*_{n-l}
\end{equation}
where $l$ is the integer time lag.

\gls{NAC} is defined as
\begin{equation}
  {\text{NAC}(\mathbf{x},\mathbf{x})} = \frac{{r(\mathbf{x},\mathbf{x})}}{\sum_n \mathbf{x}_n^2} = \frac{{r(\mathbf{x},\mathbf{x})}}{{r(\mathbf{x},\mathbf{x})}_0}
\end{equation}

\gls{AC} can be computed as the inverse Fourier transform of
the \gls{PSD}.

%}}}

\section{Power Spectral Density (PSD)}
\label{sec:PSD}
%{{{

The \gls{PSD} of a digital signal (and in general, of a wide-sense
stationary discrete-time random processes) $\mathbf{x}$,
$\text{PSD}(\mathbf{x})$, is defined as the \gls{DFT} of its
\gls{AC} function $r(\mathbf{x},\mathbf{x})$:
\begin{equation}
  \text{PSD}(\mathbf{x}) = \mathcal{F}(r(\mathbf{x},\mathbf{x})) = \sum_l r(\mathbf{x},\mathbf{x})_le^{-2\pi jfl}.
\end{equation}

Notice also that:
\begin{equation}
  \text{PSD}(\mathbf{x}) = \text{CPSD}(\mathbf{x}),
\end{equation}
and (Wiener–Khinchin Theorem):
\begin{equation}
  r(\mathbf{x},\mathbf{x}) = \mathcal{F}^{-1}(\text{PSD}(\mathbf{x})).
\end{equation}

%}}}

\begin{comment}
\section{Energy Spectral Density (ESD)}
%{{{

The ESD of a signal describes the distribution of the
energy\footnote{That obviously must be finite.} of the signal over
their frequency components. For a discrete-time signal $\mathbf{x}$,
the $\text{ESD}(\mathbf{x})$ is defined as
\begin{equation}
  \text{ESD}(\mathbf{x})=|\mathbf{X}|^2=\mathbf{X}\mathbf{X}^*,
\end{equation}
where $\mathbf{X}$ the DFT of $\mathbf{x}$, and $\mathbf{X}^*$ is its
complex conjugate.

%}}}
\end{comment}

\section{Wiener-Khinching theorem}
\label{sec:WKT}
%{{{

The Wiener-Khinching theorem states that the \gls{PSD} of a wide-sense
stationary random process (its statistical properties do not change
over time) is the Fourier transform of its \gls{AC}
function. Therefore, the \gls{CC} of two wide-sense stationary random
processes is the inverse Fourier transform of the product of the
\gls{PSD} of one process and the conjugate of the \gls{PSD} of the
other. Similarly, the \gls{AC} is the inverse Fourier transform
of the \gls{PSD}.

% https://engineering.purdue.edu/~bouman/ece637/notes/pdf/WK.pdf
\begin{equation}
  S_x(e^jw)=\sum_kR_x(k)e^{-jwk}
\end{equation}

%}}}

\section{Variance}
\label{sec:variance}
%{{{

The variance of a random variable $\mathbf{x}$, denoted by
$\mathbb{V}(\mathbf{x})$ is a measurement of its dispersion. It is
defined as the expected value of the squared deviation from the mean,
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \mathbb{E}\left[(X - \mathbb{E}[X])^2 \right].
%\end{equation}
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \operatorname{Exp}\left[(X - \operatorname{Exp}[X])^2 \right].
%\end{equation}
\begin{equation}
  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
  \label{eq:variance}
\end{equation}

%}}}

\section{Covariance}
\label{sec:covariance}
%{{{

The covariance $\mathbb{C}(\mathbf{x}, \mathbf{y})$ is a measure of
how two random variables $\mathbf{x}$ and $\mathbf{y}$ change
together. In simpler terms, it tells us the direction of the linear
relationship between two variables. The covariance between two
discrete signals $\mathbf{x}$ and $\mathbf{y}$ is calculated as
%\begin{equation}
%  \text{Cov}(\textbf{x}, \textbf{y}) = \mathbb{E}[(\mathbf{x}-\overline{\mathbf{x}})(\mathbf{y}-\overline{\mathbf{y}})].
%\end{equation}
%\begin{equation}
%  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
%\end{equation}
\begin{equation}
  \mathbb{C}(\textbf{x}, \textbf{y}) = \mathbb{E}\big((\mathbf{x}-\mathbb{E}(\mathbf{x}))(\mathbf{y}-\mathbb{E}(\mathbf{y}))\big).
\end{equation}

Notice that (see Eq.~\ref{eq:variance})
\begin{equation}
  \mathbb{C}(\mathbf{x}, \mathbf{x}) = \mathbb{V}(\mathbf{x}).
\end{equation}

%}}}

\section{Covariance matrix}
\label{sec:covariance_matrix}
%{{{

The covariance matrix $\Sigma_{\overrightarrow{\mathbf{x}}}$ of a random vector $\overrightarrow{\mathbf{x}}=[\mathbf{x}_1,\cdots,\mathbf{x}_N]^T$, defined as,
\begin{equation}
  (\Sigma_{\overrightarrow{\mathbf{x}}})_{i,j}=\mathbb{C}(\mathbf{x}_i,\mathbf{x}_j),
\end{equation}
is a $N\times N$ matrix
\begin{equation}
\Sigma_{\overrightarrow{\mathbf{x}}} = 
\begin{pmatrix}
\mathbb{V}(\mathbf{x}_1) & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_p) \\
\mathbb{C}(\mathbf{x}_2, \mathbf{x}_1) & \mathbb{V}(\mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_2, \mathbf{x}_p) \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{C}(\mathbf{x}_p, \mathbf{x}_1) & \mathbb{C}(\mathbf{x}_p, \mathbf{x}_2) & \cdots & \mathbb{V}(\mathbf{x}_p)
\end{pmatrix}
\end{equation}
that express the covariance between the random variables of a random vector.

%}}}

\section{$L_2$ norm}
\label{sec:L2_norm}
%{{{

$L_2$ norm of a discrete signal $\mathbf{x}$ is defined by
\begin{equation}
  ||\mathbf{x}||_2 = \sqrt{\sum_i\mathbf{x}_i^2}.
\end{equation}
Notice that the L2 norm and the Mean Square Error (MSE) are closely
related concepts, because
\begin{equation}
  ||\mathbf{x} - \mathbf{y}||_2^2 = N\cdot\text{MSE}(\mathbf{x} - \mathbf{y}),
\end{equation}
where $N$ is the length of $\mathbf{x}$.

%}}}

