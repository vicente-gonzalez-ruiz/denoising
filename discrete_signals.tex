\chapter{Discrete signals}

A discrete signal is the result of discretizing an analog signal in
the signal domain (time, for example), producing a sequence of
(signal) samples. Discrete signals will be represented with lower-case
bold-faced symbols, such as $\mathbf{s}$. By physical reasons, we will
suppose the discrete signals are finite with length $N$, and therefore, a discrete signal $\mathbf{s}$ will be denoted as
\begin{equation}
  \mathbf{s} = \{\mathbf{s}_n\}_{n=0}^{N-1}.
\end{equation}

\section{Signal sampling}
%{{{

Discrete signal are (a priori, infinite) sequences of samples. For example, if $s(t)$ is
a analog signal that depends on time, the $n$-th signal sample is defined by
\begin{equation}
  \mathbf{s}_n = s(t)\delta(t-nT),
\end{equation}
where  $T$ is the sampling period, and
$\delta(t-nT)$ is the \emph{unit impulse function} defined by
\begin{equation}
\delta(t) =
\begin{cases}
\infty & \text{for } t = 0 \\
0 & \text{for } t \neq 0,
\end{cases}
\end{equation}
where
\begin{equation}
\int_{-\infty}^{\infty} \delta(t) \, dt = 1.
\end{equation}

Notice that, an impulse has the so-called sifting property with
respect to integration,
\begin{equation}
\int_{-\infty}^{\infty} s(t)\delta(t) \, dt = s(0),
\end{equation}
provided that $s(t)$ is continuous at $t=0$ \cite{gonzalez1992digital}.

%Notice that, by definition,
%\begin{equation}
%  \text{Sup}(\mathbf{s}) = s[],
%\end{equation}
%where $s[]$ represents to the sequence of samples.

%}}}

\section{Energy of a discrete signal}
\label{sec:energy_signal}
%{{{

The energy of a discrete signal $\mathbf{s}$ is the sum all its samples:
\begin{equation}
  E(\mathbf{s}) = \sum_{i=1}^N|\mathbf{x}_i|^2.
\end{equation}

Notice that, by definition, the energy of a digital signal is finite,
but grows with $N$.

%}}}

\section{Power of a discrete signal}
\label{sec:power_signal}
%{{{

The power of a discrete signal $\mathbf{s}$ is
\begin{equation}
  P(\mathbf{x}) = \frac{1}{N}E(\mathbf{x}),
\end{equation}
i.e., its average energy.

Notice that, compared to the energy of a signal, the use of the power
of a signal can be more convenient to avoid arithmetic overflow in
computations with long signals.

%}}}

\section{Fourier transform of a continuous signal}
\label{sec:Fourier_transform}
%{{{

The Fourier transform, usually denoted by $\mathcal{F}$, of a
continuous function $s(t)$ is
\begin{equation}
  \mathcal{F}(s) = S(w) = \int_{-\infty}^{\infty}s(t)e^{-jwt}dt
  \label{eq:FT}
\end{equation}
and its inverse, denoted by $\mathcal{F}^{-1}$ is
\begin{equation}
  s(t) = \mathcal{F}^{-1}(S(w)) = \int_{-\infty}^{\infty}S(w)e^{jtw}dw,
\end{equation}
where $w=2\pi f$ denotes angular frequency, $j=\sqrt{-1}$, and
$f$ is frequency.

Notice that, the Fourier transform of a signal $s$ is a complex signal
$S$, even if $s$ is real.

%}}}

\begin{comment}
\section{Fourier transform of a discrete signal}
\label{sec:FTDF}
%{{{

If the signal $\mathbf{s}$ is discrete, its Fourier transform, also
known as the \gls{DTFT}, is
\begin{equation}
  \mathcal{F}(\mathbf{s}) = S(e^{jw}) = \sum_{i=1}^{N}\mathbf{s}_ie^{-jwi}
\end{equation}
which is a continuous function of $w$ if $N$, and its inverse is
\begin{equation}
  \mathbf{s} = \mathcal{F}^{-1}(S(e^{jw})) = \frac{1}{2\pi}\int_{-\pi}^{\pi}S(e^{jw})e^{jwn}dw.
\end{equation}
The \acrshort{FTDF} is periodic with period $2\pi$.

% }}}
\end{comment}

\section{Discrete Fourier Transform (DFT)}
\label{sec:DFT}
%{{{

The \gls{DFT} is the Fourier transform of a discrete finite signal $\mathbf{s}$ is
\begin{equation}
  \mathcal{F}(\mathbf{s}) = \mathbf{S} = \{\mathbf{S}_f\}_{f=0}^{N-1} \quad \text{with} \quad \mathbf{S}_f=\sum_{n=0}^{N-1}\mathbf{s}_ne^{-2\pi jf\frac{n}{N}},
  \label{eq:DFT}
\end{equation}
where $f$ denotes (discrete) frequency bins. The inverse \gls{DFT} is
\begin{equation}
  \mathbf{s} = \mathcal{F}^{-1}(\mathbf{S}) \quad \text{with} \quad \mathbf{x}_n = \frac{1}{N}\sum_{f=0}^{N-1}\mathbf{S}_fe^{2\pi jn\frac{f}{N}}.
\end{equation}

\begin{comment}
Notice that the \gls{DFT} can be interpreted as sampling the
\gls{FTDF} at $N$ evenly spaced points over $[-\pi, \pi)$. So, the
\gls{DFT} is a discrete approximation of the \gls{FTDF} for
finite-length signals.
\end{comment}

%}}}

\section{Parseval's theorem}
\label{sec:parseval}
%{{{

The Parseval's theorem for discrete signals states that, except for a
scale factor, the total energy of a signal in the signal domain (time,
for example) is equal to the total energy of its representation in the
frequency domain. Concretely, if $\mathbf{S}$ is the \gls{DFT} of the
discrete (and finte) signal $\mathbf{s}$, the Parseval’s theorem
states that
\begin{equation}
  E(\mathbf{s}) = \frac{1}{N}E(\mathbf{S}),
\end{equation}
where $E(\cdot)$ represents the energy operator (see
Section~\ref{sec:energy_signal}).

%}}}

\section{Fourier spectrum}
\label{sec:Fourier_spectrum}
%{{{

By definition, the Fourier
(or frequency) spectrum of $\mathbf{s}$ is the magnitude of
$\mathbf{S}$, that is
\begin{equation}
  |\mathbf{S}| = \sqrt{(\text{Re}(\mathbf{S}))^2+(\text{Im}(\mathbf{S}))^2}.
\end{equation}
As a complex function, we can also find the \emph{phase angle}, \emph{argument} or
\emph{phase spectrum} using
\begin{equation}
  \arg({\mathbf{S}}) = \text{arctan}\left(\frac{\text{Re}(\mathbf{S})}{\text{Im}(\mathbf{S})}\right),
\end{equation}
where $\text{arctan}()$ must be computed using a four-quadrant
arctangent function.

%}}}

\section{Power spectrum (PS)}
\label{sec:power_spectrum}
%{{{

The power spectrum of a discrete signal expresses shows how much power
the signal has at its different frequency bins. In other words, if
$\mathbf{s}$ is a discrete (and finite) signal, and $\mathbf{S}$ is
its Fourier transform, we have that
\begin{equation}
  \text{PS}(\mathbf{s}) = P(\mathbf{S})= \frac{1}{N}|\mathbf{S}|^2
\end{equation}

If $\mathbf{S}$ is the Fourier transform of $\mathbf{s}$, the power spectrum $\text{PS}()$ of a digital signal $\mathbf{x}$
shows how much power a signal has at different frequencies. Therefore,
if $\mathbf{X}$ is the DFT of $\mathbf{x}$ (with $N$ samples),
\begin{equation}
  \text{PS}(\mathbf{x}) = \frac{1}{N}|X|^2
\end{equation}
is the power spectrum of $\mathbf{x}$.

Notice that, for real-valued digital signals, the spectrum is
symmetric about $N/2$ (Nyquist frequency).

%}}}

\section{Convolution}
\label{sec:convolution}
%{{{

Convolution, typycally represented with the symbol $\ast$, is a
mathematical operation that combines two signals to produce a third
signal. It is often used to determine how the shape of one signal
modifies another, and in the context of digital signals convolution
models the fintering operation (one signal is the impulse response of
the filter and the other is the signal to filter).

For two digital signals $\mathbf{x}$ (the signal to filter) and
$\mathbf{h}$ (the filter coefficients), the convolution is defined as
\begin{equation}
(\mathbf{x}\ast\mathbf{h})_{n}=\sum_{k\in\text{Sup}(h)}\mathbf{x}_n-\mathbf{h}_{n-k}.
\label{eq:convolution}
\end{equation}

%}}}

\section{The convolution theorem}
\label{sec:convolution_theorem}
The convolution theorem states that the convolution of two discrete signals in
the signal domain is equivalent to the multiplication of their Fourier
transforms in the frequency domain:
\begin{equation}
  \mathbf{x}\ast\mathbf{h} = \text{inverse~DTFT}(\mathbf{X}(e^{jw}\mathbf{H}(e^{jw}) 
\end{equation}

\section{Cross-correlation (CC)}
\label{sec:cross-correlation}
%{{{

\gls{CC}, usually represented by the symbol $\circledast$, measures
the similarity between two signals $\mathbf{x}$ and $\mathbf{y}$ of
one relative to the other, as a function of a time lag
$l\in\mathbb{N}$. In the case of discrete-time signals, the \gls{CC}
is defined as
\begin{equation}
  {\text{CC}(\mathbf{x},\mathbf{y})}(l)=\sum_n{\mathbf{x}}_n \mathbf{y}^*_{n-l},
\end{equation}
where $\mathbf{y}^* _{n-l}$ is the complex conjugate\footnote{If the
  signal is real-valued, then $\mathbf{y}^*_{n-l}=\mathbf{y}_{n-l}$.}
of the signal delayed by $l$ samples, and therefore
${\text{CC}(\mathbf{x},\mathbf{y})}(l)$ can take as many different
values as possible values $l$ can take (it is a function of $l$).
When the range of $l$ is undefined, we will suppose that $l$ ranges in
$\text{Su}(\mathbf{\mathbf{y}})$.

The \gls{CC} can be normalized (\gls{NCC}) using
\begin{equation}
  \text{NCC}(\mathbf{x},\mathbf{y})=\frac{{r(\mathbf{x},\mathbf{y})}}{\sqrt{\sum_n \mathbf{x}_n^2 \sum_n \mathbf{y}_n^2}}.
\end{equation}

%}}}

\section{Cross-Power Spectral Density (CPSD)}
\label{sec:CPSD}
%{{{

The \gls{CPSD}, also known as the cross-spectrum, analyzes the
relationship between two different signals in the frequency domain. In
other words, the \gls{CPSD} quantifies the degree to which two signals
are correlated or ``statistically connected'' at specific
frequencies. A high \gls{CPSD} value at a particular frequency indicates a
strong correlation between the two signals at that frequency.

The \gls{CPSD} between two signals $\mathbf{x}$ and $\mathbf{y}$ is
the Fourier transform of the \gls{CC} between these two
signals:
\begin{equation}
  \text{CPSD}(\mathbf{x},\mathbf{y})=\mathcal{F}({\text{CC}(\mathbf{x},\mathbf{y})}).
\end{equation}

Alternatively,
\begin{equation}
  \text{CPSD}(\mathbf{x},\mathbf{y})_f=E(\mathbf{X}_f\mathbf{Y}_f^*).
\end{equation}
In other words, the \gls{CPSD} for the frequency bin of index $f$ is
the expectation of the product of the Fourier coefficients
$\mathbf{X}_f$ and $\mathbf{Y}_f^*$, for at least two (or more)
segments (or instances) of the signals.

%}}}

\section{Autocorrelation}
\label{sec:autocorrelation}
%{{{

The \gls{AC} of a digital signal is another digital signal that
measures the similarity between the signal and a time-delayed version
of itself as a function of a time lag (see
Section~\ref{sec:cross-correlation}). Essentially, the \gls{AC}
quantifies the degree to which a signal is correlated with its past or
future values. Mathematically, for a discrete-time\footnote{And
  therefore, for a digital signal.} signal $\mathbf{x}$ of infinite
length, the \gls{AC} signal at a lag $l$ is defined as:
\begin{equation}
  \text{AC}(\mathbf{x})(l)=\text{CC}(\mathbf{x},\mathbf{x})(l)=\sum_n{\mathbf{x}}_n \mathbf{x}^*_{n-l}
\end{equation}
where $l$ is the integer time lag.

\gls{NAC} is defined as
\begin{equation}
  {\text{NAC}(\mathbf{x},\mathbf{x})} = \frac{{r(\mathbf{x},\mathbf{x})}}{\sum_n \mathbf{x}_n^2} = \frac{{r(\mathbf{x},\mathbf{x})}}{{r(\mathbf{x},\mathbf{x})}_0}
\end{equation}

\gls{AC} can be computed as the inverse Fourier transform of
the \gls{PSD}.

%}}}

\section{Power Spectral Density (PSD)}
\label{sec:PSD}
%{{{

The \gls{PSD} of a digital signal (and in general, of a wide-sense
stationary discrete-time random processes) $\mathbf{x}$,
$\text{PSD}(\mathbf{x})$, is defined as the \gls{DFT} of its
\gls{AC} function $r(\mathbf{x},\mathbf{x})$:
\begin{equation}
  \text{PSD}(\mathbf{x}) = \mathcal{F}(r(\mathbf{x},\mathbf{x})) = \sum_l r(\mathbf{x},\mathbf{x})_le^{-2\pi jfl}.
\end{equation}

Notice also that:
\begin{equation}
  \text{PSD}(\mathbf{x}) = \text{CPSD}(\mathbf{x}),
\end{equation}
and (Wiener–Khinchin Theorem):
\begin{equation}
  r(\mathbf{x},\mathbf{x}) = \mathcal{F}^{-1}(\text{PSD}(\mathbf{x})).
\end{equation}

%}}}

\begin{comment}
\section{Energy Spectral Density (ESD)}
%{{{

The ESD of a signal describes the distribution of the
energy\footnote{That obviously must be finite.} of the signal over
their frequency components. For a discrete-time signal $\mathbf{x}$,
the $\text{ESD}(\mathbf{x})$ is defined as
\begin{equation}
  \text{ESD}(\mathbf{x})=|\mathbf{X}|^2=\mathbf{X}\mathbf{X}^*,
\end{equation}
where $\mathbf{X}$ the DFT of $\mathbf{x}$, and $\mathbf{X}^*$ is its
complex conjugate.

%}}}
\end{comment}

\section{Wiener-Khinching theorem}
\label{sec:WKT}
%{{{

The Wiener-Khinching theorem states that the \gls{PSD} of a wide-sense
stationary random process (its statistical properties do not change
over time) is the Fourier transform of its \gls{AC}
function. Therefore, the \gls{CC} of two wide-sense stationary random
processes is the inverse Fourier transform of the product of the
\gls{PSD} of one process and the conjugate of the \gls{PSD} of the
other. Similarly, the \gls{AC} is the inverse Fourier transform
of the \gls{PSD}.

% https://engineering.purdue.edu/~bouman/ece637/notes/pdf/WK.pdf
\begin{equation}
  S_x(e^jw)=\sum_kR_x(k)e^{-jwk}
\end{equation}

%}}}

\section{Variance}
\label{sec:variance}
%{{{

The variance of a random variable $\mathbf{x}$, denoted by
$\mathbb{V}(\mathbf{x})$ is a measurement of its dispersion. It is
defined as the expected value of the squared deviation from the mean,
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \mathbb{E}\left[(X - \mathbb{E}[X])^2 \right].
%\end{equation}
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \operatorname{Exp}\left[(X - \operatorname{Exp}[X])^2 \right].
%\end{equation}
\begin{equation}
  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
  \label{eq:variance}
\end{equation}

%}}}

\section{Covariance}
\label{sec:covariance}
%{{{

The covariance $\mathbb{C}(\mathbf{x}, \mathbf{y})$ is a measure of
how two random variables $\mathbf{x}$ and $\mathbf{y}$ change
together. In simpler terms, it tells us the direction of the linear
relationship between two variables. The covariance between two
discrete signals $\mathbf{x}$ and $\mathbf{y}$ is calculated as
%\begin{equation}
%  \text{Cov}(\textbf{x}, \textbf{y}) = \mathbb{E}[(\mathbf{x}-\overline{\mathbf{x}})(\mathbf{y}-\overline{\mathbf{y}})].
%\end{equation}
%\begin{equation}
%  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
%\end{equation}
\begin{equation}
  \mathbb{C}(\textbf{x}, \textbf{y}) = \mathbb{E}\big((\mathbf{x}-\mathbb{E}(\mathbf{x}))(\mathbf{y}-\mathbb{E}(\mathbf{y}))\big).
\end{equation}

Notice that (see Eq.~\ref{eq:variance})
\begin{equation}
  \mathbb{C}(\mathbf{x}, \mathbf{x}) = \mathbb{V}(\mathbf{x}).
\end{equation}

%}}}

\section{Covariance matrix}
\label{sec:covariance_matrix}
%{{{

The covariance matrix $\Sigma_{\overrightarrow{\mathbf{x}}}$ of a random vector $\overrightarrow{\mathbf{x}}=[\mathbf{x}_1,\cdots,\mathbf{x}_N]^T$, defined as,
\begin{equation}
  (\Sigma_{\overrightarrow{\mathbf{x}}})_{i,j}=\mathbb{C}(\mathbf{x}_i,\mathbf{x}_j),
\end{equation}
is a $N\times N$ matrix
\begin{equation}
\Sigma_{\overrightarrow{\mathbf{x}}} = 
\begin{pmatrix}
\mathbb{V}(\mathbf{x}_1) & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_p) \\
\mathbb{C}(\mathbf{x}_2, \mathbf{x}_1) & \mathbb{V}(\mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_2, \mathbf{x}_p) \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{C}(\mathbf{x}_p, \mathbf{x}_1) & \mathbb{C}(\mathbf{x}_p, \mathbf{x}_2) & \cdots & \mathbb{V}(\mathbf{x}_p)
\end{pmatrix}
\end{equation}
that express the covariance between the random variables of a random vector.

%}}}

\section{$L_2$ norm}
\label{sec:L2_norm}
%{{{

$L_2$ norm of a discrete signal $\mathbf{x}$ is defined by
\begin{equation}
  ||\mathbf{x}||_2 = \sqrt{\sum_i\mathbf{x}_i^2}.
\end{equation}
Notice that the L2 norm and the Mean Square Error (MSE) are closely
related concepts, because
\begin{equation}
  ||\mathbf{x} - \mathbf{y}||_2^2 = N\cdot\text{MSE}(\mathbf{x} - \mathbf{y}),
\end{equation}
where $N$ is the length of $\mathbf{x}$.

%}}}

