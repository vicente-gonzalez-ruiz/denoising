\chapter{Discrete signals}

A discrete signal is the result of discretizing an analog signal in
the signal domain (time, for example), producing a sequence of
(signal) samples. Discrete signals will be represented with lower-case
bold-faced symbols, such as $\mathbf{s}$. By physical reasons, we will
suppose the discrete signals are finite with length $N$, and
therefore, a discrete signal $\mathbf{s}$ will be denoted as
\begin{equation}
  \mathbf{s} = \{\mathbf{s}_n\}_{n=0}^{N-1}.
\end{equation}
However, notice that the assumption of this constrain ($N$ finite)
only obey to the idea of focusing our analysis into the
\emph{expected} (and most logical) case. In other words, a similar
presentation could be done considering infinite discrete signals. In
some results, we will see what happens.

\section{Signal sampling}
%{{{

Discrete signal are (a priori, infinite) sequences of samples. For example, if $s(t)$ is
a analog signal that depends on time, the $n$-th signal sample is defined by
\begin{equation}
  \mathbf{s}_n = s(t)\delta(t-nT),
\end{equation}
where  $T$ is the sampling period, and
$\delta(t-nT)$ is the \emph{unit impulse function} defined by
\begin{equation}
\delta(t) =
\begin{cases}
\infty & \text{for } t = 0 \\
0 & \text{for } t \neq 0,
\end{cases}
\end{equation}
where
\begin{equation}
\int_{-\infty}^{\infty} \delta(t) \, dt = 1.
\end{equation}

Notice that, an impulse has the so-called sifting property with
respect to integration,
\begin{equation}
\int_{-\infty}^{\infty} s(t)\delta(t) \, dt = s(0),
\end{equation}
provided that $s(t)$ is continuous at $t=0$ \cite{gonzalez1992digital}.

%Notice that, by definition,
%\begin{equation}
%  \text{Sup}(\mathbf{s}) = s[],
%\end{equation}
%where $s[]$ represents to the sequence of samples.

%}}}

\section{Energy of a discrete signal}
\label{sec:energy_signal}
%{{{

The energy of a discrete signal $\mathbf{s}$ is the sum all its samples:
\begin{equation}
  E(\mathbf{s}) = \sum_{i=0}^{N-1}|\mathbf{s}_i|^2.
\end{equation}

Notice that, by definition, the energy of a finite-length discrete
signal is finite, but grows with $N$. Obviously, if $N$ is infinite,
the energy can be also infinite.

%}}}

\section{Power of a discrete signal}
\label{sec:power_signal}
%{{{

The power of a discrete signal $\mathbf{s}$ is
\begin{equation}
  P(\mathbf{s}) = \frac{1}{N}E(\mathbf{s}),
  \label{eq:power_discrete_signal}
\end{equation}
i.e., its average energy (average of the squared magnitude of the
signal values).

Notice that, compared to the energy of a signal, the use of the power
of a signal can be more convenient to avoid arithmetic overflow in
computations with long signals, and this is true even if
$N\rightarrow\infty$.

%}}}

\section{Fourier transform of a continuous signal}
\label{sec:Fourier_transform}
%{{{

The Fourier transform, usually denoted by $\mathcal{F}$, of a
continuous function $s(t)$ is
\begin{equation}
  \mathcal{F}(s) = S(w) = \int_{-\infty}^{\infty}s(t)e^{-jwt}dt
  \label{eq:FT}
\end{equation}
and its inverse, denoted by $\mathcal{F}^{-1}$ is
\begin{equation}
  s(t) = \mathcal{F}^{-1}(S(w)) = \int_{-\infty}^{\infty}S(w)e^{jtw}dw,
\end{equation}
where $w=2\pi f$ denotes angular frequency, $j=\sqrt{-1}$, and
$f$ is frequency.

Notice that, the Fourier transform of a signal $s$ is a complex signal
$S$, even if $s$ is real.

%}}}

\section{Discrete Fourier Transform (DFT)}
\label{sec:DFT}
%{{{

The \gls{DFT} is the Fourier transform of a discrete finite signal $\mathbf{s}$ is
\begin{equation}
  \mathcal{F}(\mathbf{s}) = \mathbf{S} = \{\mathbf{S}_f\}_{f=0}^{N-1} \quad \text{with} \quad \mathbf{S}_f=\sum_{n=0}^{N-1}\mathbf{s}_ne^{-2\pi jf\frac{n}{N}},
  \label{eq:DFT}
\end{equation}
where $f$ denotes (discrete) frequency bins. The inverse \gls{DFT} is
\begin{equation}
  \mathbf{s} = \mathcal{F}^{-1}(\mathbf{S}) \quad \text{with} \quad \mathbf{x}_n = \frac{1}{N}\sum_{f=0}^{N-1}\mathbf{S}_fe^{2\pi jn\frac{f}{N}}.
\end{equation}

The \gls{DFT} can be interpreted as sampling the \gls{FTDF} (see
Appendix~\ref{sec:FTDF}) at $N$ evenly spaced points over
$[-\pi, \pi)$. So, the \gls{DFT} is a discrete approximation of the
\gls{FTDF} for finite-length signals. If $N$ were infinite, what
happens is that the frequency bins would be infinitesimally narrow,
and the Fourier transform of the discrete signal would be continuous.

Finally, there exists a faster version of the \gls{DFT} (with have a
complexity of $\mathcal{O}^2$ called \gls{FFT} with complexity
$\mathcal{O}\log_2\mathcal{O}$.

%}}}

\section{Parseval's theorem}
\label{sec:parseval}
%{{{

The Parseval's theorem for discrete signals states that, except for a
scale factor, the total energy of a signal in the signal domain (time,
for example) is equal to the total energy of its representation in the
frequency domain. Concretely, if $\mathbf{S}$ is the \gls{DFT} of the
discrete (and finte) signal $\mathbf{s}$, the Parsevalâ€™s theorem
states that
\begin{equation}
  E(\mathbf{s}) = \frac{1}{N}E(\mathbf{S}),
\end{equation}
where $E(\cdot)$ represents the energy operator (see
Section~\ref{sec:energy_signal}).

%}}}

\section{Fourier spectrum}
\label{sec:Fourier_spectrum}
%{{{

By definition, the Fourier
(or frequency) spectrum of $\mathbf{s}$ is the magnitude of
$\mathbf{S}$, that is
\begin{equation}
  |\mathbf{S}| = \sqrt{(\text{Re}(\mathbf{S}))^2+(\text{Im}(\mathbf{S}))^2}.
\end{equation}
As a complex function, we can also find the \emph{phase angle}, \emph{argument} or
\emph{phase spectrum} using
\begin{equation}
  \arg({\mathbf{S}}) = \text{arctan}\left(\frac{\text{Re}(\mathbf{S})}{\text{Im}(\mathbf{S})}\right),
\end{equation}
where $\text{arctan}()$ must be computed using a four-quadrant
arctangent function.

%}}}

\section{Power spectrum (PS)}
\label{sec:power_spectrum}
%{{{

The power spectrum $\text{PS}()$ of finite-length discrete signal
$\mathbf{s}$ shows how much power (see Section~\ref{sec:power_signal})
a signal has at its different frequency bins. Let $\mathbf{S}$ the
Fourier transform of $\mathbf{s}$. Then,
\begin{equation}
  \text{PS}(\mathbf{s}) = \{\text{PS}(\mathbf{s})_f\}_{f=0}^{N-1} \quad \text{where} \quad \text{PS}(\mathbf{s})_f = \frac{1}{N}|\mathbf{S}_f|^2,
\end{equation}
where $f$ denotes the frequency bin.

Notice that, for real-valued digital signals, the spectrum is
symmetric about $N/2$ (Nyquist frequency).

%}}}

\section{Convolution}
\label{sec:convolution}
%{{{

Convolution, typically represented with the symbol $\ast$, is a
mathematical operation that combines two signals to produce a third
signal. From a physical perspective, convolution models how a signal
is transformed by a system's behavior what is itself described by
another signal (usually, the response of the system to the an
impulse). An example of such system can be a digital filter.

For two signals $\mathbf{s}$ (the signal to filter) and
$\mathbf{h}$ (the filter coefficients), the convolution is defined as
\begin{equation}
(\mathbf{s}\ast\mathbf{h})_{n}=\sum_{k=0}^{K-1}\mathbf{s}_{n-k}\mathbf{h}_{k},
\label{eq:convolution}
\end{equation}
where $K$ is the number of coefficients (of the filter), and $n$ is
known as the \emph{lag} (where the filter is applied in $\mathbf{s}$).

Notice that we could need to extend $\mathbf{s}$ to convolve.

%}}}

\section{Transfer function of a filter}
\label{sec:TF_filter}

The \gls{TF} of a (\gls{LTI}) filter describes how the filter modifies
the amplitude and phase of each frequency component of the input
signal. In the analog world, it is defined as
\begin{equation}
  H(s) = \frac{Y(s)}{X(s)},
\end{equation}
where $Y(s)$ is the Laplace transform of the output, $X(s)$ is the
Laplace transform of the input, and $s$ represents a complex
frequency. In the digital case, the Laplace-domain is changed by the
Z-domain, resulting (\gls{IIR} filter)
\begin{equation}
  H(z) = \frac{Y(z)}{X(z)} = \frac{b_0 + b_1 z^{-1} + b_2 z^{-2} + \dots + b_M z^{-M}}{1 + a_1 z^{-1} + a_2 z^{-2} + \dots + a_N z^{-N}}
\end{equation}
where $Y(z)$ is the Z-transform of the output, $X(z)$ is the
Z-transform of the input, $\{b_k\}$ are the feedforward coefficients,
and $\{a_k\}$ are the feedback coefficients. In the \gls{FIR} case,
all $a_k=0$ for $k\ge 1$, and therefore
\begin{equation}
  H(z) = b_0 + b_1 z^{-1} + b_2 z^{-2} + \dots + b_M z^{-M}.
\end{equation}

In both analog and digital cases, evaluating the transfer function on
the imaginary axis or unit circle, respectively, gives the frequency
response: $H(j\omega)$ (analog) $H(e^{j\omega})$ (digital).

\section{FFT-based convolution (the convolution theorem)}
\label{sec:convolution_theorem}
%{{{

The convolution theorem states that the convolution of two signals in
the signal domain is equivalent to the (coefficient-wise)
multiplication of their Fourier transforms and then performs the
inverse Fourier transform:
\begin{equation}
  \mathbf{x}\ast\mathbf{h} = \mathcal{F}^{-1}(\mathbf{X}\mathbf{H}).
\end{equation}

Conversely, the Fourier transform of the product of two signals is
the convolution of their individual Fourier transforms:
\begin{equation}
  \mathcal{F}(\mathbf{x}\mathbf{h}) = \mathbf{X}\ast\mathbf{H}.
\end{equation}

We can compute the convolution in the Fourier
domain with a
complexity of $O(N\log N)$, using the following steps:
\begin{enumerate}
\item Take the FFT of $\mathbf{x}$ ($O(N\log N)$):
  \begin{equation}
    \mathbf{X} = \mathcal{F}(x).
  \end{equation}
\item Take the FFT of $\mathbf{y}$ ($O(N\log N)$):
  \begin{equation}
    \mathbf{Y} = \mathcal{F}(y).
  \end{equation}
\item Multiply (element-wise) $\mathbf{X}$ by
  $\mathbf{Y}$:
  \begin{equation}
    \mathcal{F}(\mathbf{x}\ast\mathbf{y})=\mathbf{X}\mathbf{Y}.
  \end{equation}
\item Take the inverse FFT of the result to get the convolution
  function ($O(N\log N)$):
  \begin{equation}
    \mathbf{x}\ast\mathbf{y} = \mathcal{F}^{-1}(\mathcal{F}(\mathbf{x}\ast\mathbf{y}))
  \end{equation}
\end{enumerate}

Notice that to perform a convolution in the Fourier domain, both
signals must have the same shape, which likely will force to zero-pad
$\mathbf{h}$. Therefore, when $K$ iw big, this can speed-up the
convolution if we use \gls{FFT} algorithms.

%}}}

\section{Cross-correlation (CC)}
\label{sec:cross-correlation}
%{{{

Essentially, \gls{CC}, usually represented by the symbol
$\circledast$, is the measure of how two or more variables are related
to one another. In the context of this chapter, \gls{CC} can be used
to measure the similarity between two signals $\mathbf{s}$ and
$\mathbf{h}$ (both usually with the same shape) of one relative to the
other, as a function of a time lag $n\in\{n\}_{n=0}^{N-1}$. The
\gls{CC} is defined as
\begin{equation}
  (\mathbf{s}\circledast\mathbf{h})_n=\sum_{k=0}^{N-1}{\mathbf{s}}_{n+k} \mathbf{h}_n.
  \label{eq:cross-correlation}
\end{equation}

Notice that the \gls{CC} is identical to the convolution except that
one of the signals is \emph{flipped} ($\mathbf{s}$ in our case).

The \gls{CC} can be normalized between 0 and 1 (\gls{NCC}) using
\begin{equation}
  \text{NCC}(\mathbf{s},\mathbf{h})=\frac{{\mathbf{s}\circledast\mathbf{h}}}{\sqrt{\sum_n \mathbf{s}_n^2 \sum_n \mathbf{h}_n^2}}.
\end{equation}

%}}}

\section{Cross-Power Spectral Density (CPSD)}
\label{sec:CPSD}
%{{{

The \gls{CPSD}, also known as cross-spectrum, analyzes the
relationship between two different signals in the Fourier
domain. Therefore, the \gls{CPSD} quantifies the degree to which two
signals are correlated or ``statistically connected'' at specific
frequencies (a high \gls{CPSD} value at a particular frequency
indicates a strong correlation between the two signals at that
frequency).

The \gls{CPSD} between two signals $\mathbf{x}$ and $\mathbf{y}$ is
the Fourier transform of the \gls{CC} between these two
signals:
\begin{equation}
  \text{CPSD}(\mathbf{x},\mathbf{y})=\mathcal{F}({\mathbf{x}\circledast\mathbf{y}}).
\end{equation}

%Alternatively,
%\begin{equation}
%  \text{CPSD}(\mathbf{x},\mathbf{y})_f = \mathbb{E}(\mathbf{X}_f\mathbf{Y}_f^*).
%\end{equation}
%In other words, the \gls{CPSD} for the frequency bin of index $f$ is
%the expectation of the product of the Fourier coefficients
%$\mathbf{X}_f$ and $\mathbf{Y}_f^*$, for at least two (or more)
%segments (or instances) of the signals.

%}}}

\section{FFT-based cross-correlation (the cross-correlation theorem)}
%{{{

See the convolution in the Fourier fomain (Section~\ref{sec:convolution_theorem}):
\begin{equation}
  \mathbf{x}\circledast\mathbf{h} = \mathcal{F}^{-1}(\mathbf{X}\mathbf{H}^*),
  \label{eq:FFT_CC}
\end{equation}
where $\mathbf{H}^*$ is the complex conjugate of $\mathbf{H}$.

We can compute the cross-correlation in the Fourier
domain with a
complexity of $O(N\log N)$, using the following steps:
\begin{enumerate}
\item Take the FFT of $\mathbf{x}$ ($O(N\log N)$):
  \begin{equation}
    \mathbf{X} = \mathcal{F}(x).
  \end{equation}
\item Take the FFT of $\mathbf{y}$ ($O(N\log N)$):
  \begin{equation}
    \mathbf{Y} = \mathcal{F}(y).
  \end{equation}
\item Multiply (element-wise) $\mathbf{X}$ by the complex conjugate of
  $\mathbf{Y}$ to get the CPSD (Cross-Power Spectral Density) ($O(N)$):
  \begin{equation}
    \text{CPSD}(\mathbf{x},\mathbf{y})=\mathcal{F}(\mathbf{x}\circledast\mathbf{y})=\mathbf{X}\mathbf{Y}^*
  \end{equation}
\item Take the inverse FFT of the result to get the cross-correlation
  function ($O(N\log N)$):
  \begin{equation}
    \mathbf{PCC} = \mathcal{F}^{-1}(\text{CPSD}(\mathbf{x},\mathbf{y}))
  \end{equation}
\end{enumerate}

%}}}

\section{Auto-correlation}
\label{sec:auto-correlation}
%{{{

\gls{AC} measures the similarity (through a \gls{CC} operation)
between a signal and a shifted version of it. Therefore (see
Eq.~\ref{eq:cross-correlation}),
\begin{equation}
  \text{AC}(\mathbf{s})_n = (\mathbf{s}\circledast\mathbf{s})_n.
\end{equation}

Notice that if we are insterested in the \gls{AC} for all the possible
lag values, Eq.~\ref{eq:FFT_CC} offers an efficient technique.

%The \gls{AC} of a digital signal is another digital signal that
%measures the similarity between the signal and a time-delayed version
%of itself as a function of a time lag (see
%Section~\ref{sec:cross-correlation}). Essentially, the \gls{AC}
%quantifies the degree to which a signal is correlated with its past or
%future values. Mathematically, for a discrete-time\footnote{And
%  therefore, for a digital signal.} signal $\mathbf{x}$ of infinite
%length, the \gls{AC} signal at a lag $l$ is defined as:
%\begin{equation}
%  \text{AC}(\mathbf{x})(l)=\text{CC}(\mathbf{x},\mathbf{x})(l)=\sum_n{\mathbf{x}}_n \mathbf{x}^*_{n-l}
%\end{equation}
%where $l$ is the integer time lag.

%\gls{NAC} is defined as
%\begin{equation}
%  {\text{NAC}(\mathbf{x},\mathbf{x})} = \frac{{r(\mathbf{x},\mathbf{x})}}{\sum_n \mathbf{x}_n^2} = \frac{{r(\mathbf{x},\mathbf{x})}}{{r(\mathbf{x},\mathbf{x})}_0}
%\end{equation}

%\gls{AC} can be computed as the inverse Fourier transform of
%the \gls{PSD}.

%}}}

\section{Power Spectral Density (PSD)}
\label{sec:PSD}
%{{{

As happens with the \gls{CC} and the \gls{AC}, the \gls{PSD} of a
signal $\mathbf{x}$ is the Fourier transform of its \gls{AC}:
\begin{equation}
  \text{PSD}(\mathbf{x}) = \mathcal{F}(\mathbf{x}\circledast\mathbf{x}).
\end{equation}
%}}}

\section{Wiener-Khinching theorem}
\label{sec:WKT}
%{{{

The Wiener-Khinching theorem states that the \gls{PSD} of a signal is
the Fourier transform of its autocorrelation function:
\begin{equation}
  \text{PSD}(\mathbf{s}) = \mathcal{F}(\text{AC}(\mathbf{s})).
\end{equation}
Therefore, taking the inverse Fourier transform on both sides,
\begin{equation}
  \mathcal{F}^{-1}(\text{PSD}(\mathbf{s})) = \text{AC}(\mathbf{s}),
\end{equation}
which matches the Eq.~\ref{eq:FFT_CC}.

%Notice also that:
%\begin{equation}
%  \text{PSD}(\mathbf{x}) = \text{CPSD}(\mathbf{x}),
%\end{equation}
%and (Wienerâ€“Khinchin Theorem):
%\begin{equation}
%  r(\mathbf{x},\mathbf{x}) = \mathcal{F}^{-1}(\text{PSD}(\mathbf{x})).
%\end{equation}


%The Wiener-Khinching theorem states that the \gls{PSD} of a wide-sense
%stationary random process (its statistical properties do not change
%over time) is the Fourier transform of its \gls{AC}
%function. Therefore, the \gls{CC} of two wide-sense stationary random
%processes is the inverse Fourier transform of the product of the
%\gls{PSD} of one process and the conjugate of the \gls{PSD} of the
%other. Similarly, the \gls{AC} is the inverse Fourier transform
%of the \gls{PSD}.

% https://engineering.purdue.edu/~bouman/ece637/notes/pdf/WK.pdf
%\begin{equation}
%  S_x(e^jw)=\sum_kR_x(k)e^{-jwk}
%\end{equation}

%}}}

\begin{subappendices}

\begin{comment}
\section{Energy Spectral Density (ESD)}
%{{{

The ESD of a signal describes the distribution of the
energy\footnote{That obviously must be finite.} of the signal over
their frequency components. For a discrete-time signal $\mathbf{x}$,
the $\text{ESD}(\mathbf{x})$ is defined as
\begin{equation}
  \text{ESD}(\mathbf{x})=|\mathbf{X}|^2=\mathbf{X}\mathbf{X}^*,
\end{equation}
where $\mathbf{X}$ the DFT of $\mathbf{x}$, and $\mathbf{X}^*$ is its
complex conjugate.

%}}}
\end{comment}

\section{Fourier transform of a discrete signal}
\label{sec:FTDF}
%{{{

If the signal $\mathbf{s}$ is discrete, its Fourier transform, also
known as the \gls{DTFT}, is
\begin{equation}
  \mathcal{F}(\mathbf{s}) = S(e^{jw}) = \sum_{i=1}^{N}\mathbf{s}_ie^{-jwi}
\end{equation}
which is a continuous function of $w$ if $N$, and its inverse is
\begin{equation}
  \mathbf{s} = \mathcal{F}^{-1}(S(e^{jw})) = \frac{1}{2\pi}\int_{-\pi}^{\pi}S(e^{jw})e^{jwn}dw.
\end{equation}
The \acrshort{FTDF} is periodic with period $2\pi$.

%}}}

\section{Dot product of two discrete signals}
The dot product of two signal $\mathbf{A}$ and $\mathbf{B}$ is
calculated multiplying the corresponding elements of both signals and
summing the result:
\begin{equation}
  \mathbf{A}\cdot\mathbf{B} = \sum_{i=0}^{n-1}\mathbf{A}_i\mathbf{B_i}.
\end{equation}

\end{subappendices}

