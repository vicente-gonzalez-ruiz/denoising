\chapter{Introduction}

Image denoising, which is an inverse problem, is a significant
research topic in computer vision, aiming to restore a latent clean
image from an observed image degraded by
noise. %
       % https://www.sciencedirect.com/science/article/pii/S0950705125016326


Image denosing if a restoration is the problem in which an image is
reconstructed from a corrupted version of itself
\cite{BUCHHOLZ2019277}.

Iqmage noise primarily arises during data acquisition, stemming from
sensor electronics, photon statistics, and variations in imaging
conditions such as illumination and scene dynamics. The resulting
degradations can be stochastic (e.g., Poisson or Gaussian) or
structured (e.g., speckle or banding), both of which reduce the
signal-to-noise ratio, impair visual quality, and hinder downstream
analysis. Denoising methods are therefore needed to recover the
underlying clean image, either by modeling the corruption process or
by exploiting image priors such as self-similarity and
sparsity. % https://arxiv.org/pdf/2510.01666

Conventional denoising methods typically assume that noise follows a
known statistical model, such as a Gaussian distribution or a mixture
of well-characterized distributions. These assumptions allow
classical methods to perform noise removal in a principled
manner. However, most real-world noise is considerably more complex,
being neither independent nor accurately represented by analytic
models.  % https://arxiv.org/pdf/2510.01666

\section{Why microscopy?}
Microscopy of biological specimens is used in biotechnology to
visualize small specimens (including their internal structures) that
cannot be seen with the naked eye. For this, some kind of interaction
between the specimens and a source of energy must exist (the specimen
must be irradiated), an action that usually degrades the specimen and
for this reason, the radiation is minimized (in radiation power or
``dwell'' time) generating low SNR images. In this report, we describe,
evaluate, and compare several image denoising algorithms commonly used
in microscopy of biological specimens.

\section{Image acquisition in microscopy}
%{{{

There are several tecniques to capture microscopy images:

\begin{enumerate}
  \item \textbf{\gls{LM}}: Light passes through the specimen, and this light
  is then magnified by an objective and ocular lenses to form an
  observable image. Fluorescence microscopy and confocal microscopy
  are two \gls{LM} tecniques, where the specimen(s) emit light after
  have being excited by a source of light. Using \gls{LM} we can
  typical reach a resolution of up to 200 nm.

\item \textbf{\gls{EM}}: We use a beam of electrons that can pass
  through the sample to discover the internal structure (\gls{TEM}) or
  bounce on the sample, in this case to analize the surface topology
  (\gls{SEM}) \cite{timischl2012statistical}. X-rays and ions can also
  be used (the smaller the wavelength of the radiation, the higher the
  resolution). In general, compared to \gls{LM}, \gls{EM} allows much
  higher resolution (down to sub-nanometer scale).

\item \textbf{\gls{SPM}}: A nano-scale mechanical probe maps the surface of a
  specimen. \gls{SPM} achieves sub-nanometer resolution, allowing for
  the imaging of surfaces at the atomic scale. The \gls{AFM} and the
  \gls{STM} are \gls{SPM} techniques.

\end{enumerate}
  
%}}}

\section{Tomography}
%{{{

Tomography refers to the computational reconstruction of 3D volumes
from a series of 2D projections. Depending on the imaging modalities
we can distinguish bewtween:
\begin{enumerate}
\item \textbf{\gls{ET}}: Used mainly in cell biology and materials
  science. The samples are usually resin-embedded. Achieve resolutions
  btween 1 and 10 nm.
\item \textbf{\gls{cryo-ET}}: Used in structural biology (e.g.,
  visualizing viruses, ribosomes, organelles). To minimize the damages
  in the samples they are frozen-hydrated using a A flash-freezing
  technique, called vitrification, which suspends the specimens in a
  thin layer of amorphous, non-crystaline ice\footnote{Which prevents
    the formation of damaging ice crystals and therefore preserving
    the sample's structure.}. Achieven resolutions range between 3 and
  5 nm. X-rays can replace electron-beams.
\item \textbf{\gls{OPT}}: Usually based in
  light microscopy, OPT is a non-destructive 3D imaging technique used
  to observe transparent samples like embryos or organoids. The
  typical resolutions range between 5 and 50 $\mu$m.
\item \textbf{Fluorescence Tomography}: Used to mapping 3D
  fluorescence distribution in tissues, often for molecular imaging in
  live animals or cleared tissues. Resolution ranges between 10 and
  100 $\mu$m.
\item \textbf{\gls{SXT}}: 3D imaging of intact cells with
  higher penetration than electrons or light. Resolution between 30
  and 50 nm.
\item \textbf{\gls{PAT}}: Light excitation + ultrasound
  detection. Used in deep-tissue imaging of functional contrast (like
  blood oxygenation) in live animals. Resolution between 50 and 300
  $\mu$m.
\end{enumerate}

Therefore, a tomogram is a 3D digital signal (volume) generated by a
tomography technique.

% }}}

\section{Distortion microscopy images}
Typical distortions can include the effects of camera and
photon/electron noise, the blur of the optical point-spread-function,
resolution loss due to sampling, or aberrations induced by the sample
itself. Although we have a very detailed understanding of the nature
and physics behind contributing distortions, it is hard to compute the
inverse, i.e., generate a reconstructed image by only observing the
corrupted images acquired by a microscopes. The inverse of an observed
image x is usually not uniquely defined, meaning that multiple
undistorted images could have given rise to the corrupted version we
observed. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

Several of these distortion are non-invertible problems.

\section{A general solution to non-invertible problems}
Such non-invertible problems are typically approached by minimizing a
cost function
\begin{equation}
y^*=\underset{y}{\operatorname{arg\,min}} \, E(x,y),
\end{equation}
where
\begin{equation}
E = D(x,y) + \lambda R(y),
\end{equation}
combines a data term $D$ and a regularizer term $R$. The regularizer
favors solutions based on prior assumptions about uncorrupted images,
and therefore, it helps to pick one restored image $y^*$ from the set
of uncorrupted images that might have given rise to $x$. The data term
D encourages the restored image to be in-line with the observation and
our understanding of the physical nature of distortions. Dependent on
the severity of the distortion, regularization has to be tuned by
weighing it with a parameter $\lambda$. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

The regularization term R traditionally encodes rather simple,
hand-crafted assumptions, often as simple as favoring smooth images by
penalizing strong differences of neighboring pixels.  % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Content-Aware image REstoration (CARE)}

Image restoration is the problem of reconstructing an image from a corrupted version of itself. % https://www.sciencedirect.com/science/article/abs/pii/S0091679X19300706?via%3Dihub

\section{Noise in microscopy}
%{{{

In microscopy imaging there are three main sources of noise: (1)
\emph{dark noise} which corresponds to the electronic noise generated
by the thermal agitation of electrons, (2) \emph{photon noise} (or
shot noise) that is generated by the fluctuations in the number of
photons sensed at a given signal exposure level, and (3) the
\emph{read-out noise}, generated by the non-perfectness of the output
electronic amplifiers used in the process of converting photon or
electron accumulations into voltages. Therefore, the level of noise
depends on the exposure time and experimental conditions affecting the
sensors such as temperature, among other capturing parameters. Dark
and photon noises are usually modeled as a Poisson distribution
$\mathcal{P}(\lambda)$, where $\lambda$ represents the average dark
flux. Typically, read-out noise is modeled as zero-mean additive white Gaussian
noise \cite{meiniel2018denoising,zhou2020wirtinger}.

%}}}

\section{Why denoising?}
\label{sec:why_denoising}
%{{{

%We consider denoising techniques where there is only one noisy
%instance or a few instances, usually in the latter case with slightly
%different versions of the clean signal. Averaging as such is therefore
%excluded.

In general, when we study biological samples, the impact of the
ratiation of generated by the microscope can degrade it. To minimize
this degradation, the radiation doses are minimized, generating low
\gls{SNR} images which require the use of denoising algorithms to
facilitate subsequent analysis. An optimal denoiser should
effectively attenuate noise while concurrently preserving crucial
image features, including edges, textures, and underlying biological
structures.

An added challenge to this situation is the limited availability of
only a single noisy realization of these images, which complicates
both the evaluation of denoising algorithms (due to the absence of a
ground truth for comparative assessment) and the selection of
appropriate denoising parameters. 

%}}}

\section{Denoisers classification}

\begin{enumerate}
  
\item Planar imaging-based denoising methods, that only a single 2D image to be applied.

  % Z. Y. M. Q. e. a. Qiao, C., “Zero-shot learning enables instant denoising
  % and super-resolution in optical fluorescence microscopy,” Nature Com-
  % munications, vol. 15, no. 4180, 2024.

  % P. R. S. A. e. a. Lequyer, J., “A fast blind zero-shot denoiser,” Nature
  % Machine Intelligence, vol. 4, p. 953–963, 2022.

\item Time-lapse imaging-based denoising methods, which leverage the
  temporal information present across multiple frames to distinguish
  between actual signal and random noise.
  
  % X. Li, G. Zhang, J. Wu, Y. Zhang, Z. Zhao, X. Lin, H. Qiao, H. Xie,
  % H. Wang, L. Fang et al., “Reinforcing neuron extraction and spike
  % inference in calcium imaging using deep self-supervised denoising,”
  % Nature Methods, pp. 1–6, 2021.

  % . H. S. N. O. P. L. . C. K. J´erˆome Lecoq, Michael Oliver, “Removing in-
  % dependent noise in systems neuroscience data using deepinterpolation,”
  % Nature Methods, vol. 18, pp. 1401–1408, 2021.

\end{enumerate}

\section{Constructing noisy images from a single noisy image}

