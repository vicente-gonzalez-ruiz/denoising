\chapter{Signals}
%{{{

Mathematically, we can model signals as random variables. A random
variable is a mathematical formalization of a quantity or object which
depends on random events. When more than one value (a vector) is
generated in one of these envents, we can establish a similar
connection between multicomponent signals and random vectors. Random
variables (and therefore monocomponent signals) will be denoted with
bold-faced symbols, such as $\mathbf{x}$. Random vectors
(multicomponent signals) will be denoted as
$\overrightarrow{\mathbf{x}}$.

Statistically, random variables can be described throught the mean and
the variance and the corresponding distribution. Random vectors
require a mean for each component, a covariance matrix, and a
multivariate distribution.

\section{Signal sampling}
%{{{

A signal sample $\mathbf{s}_n$ is created from the signal $s(t)$ using
\begin{equation}
  \mathbf{s}_n = s(t)\delta(t-nT),
\end{equation}
where $n$ represents the sample index, $T$ is the sampling period, and
$\delta(t-nT)$ is the \emph{unit impulse function} defined by
\begin{equation}
\delta(t) =
\begin{cases}
\infty & \text{for } t = 0 \\
0 & \text{for } t \neq 0,
\end{cases}
\end{equation}
where
\begin{equation}
\int_{-\infty}^{\infty} \delta(t) \, dt = 1.
\end{equation}

An impulse has the so-called sifting
property with respect to integration,
\begin{equation}
\int_{-\infty}^{\infty} s(t)\delta(t) \, dt = s(0),
\end{equation}
provided that $s(t)$ is continuous at $t=0$ \cite{gonzalez1992digital}.

%}}}

\section{Energy of a signal (Parseval's theorem)}
%{{{

By definition, the energy $E()$ of a digital (discrete and finite) signal
$\mathbf{x}$ of length $N$ is
\begin{equation}
  E(\mathbf{x}) = \sum_{i}|\mathbf{x}_i|^2.
\end{equation}

Notice that, by definition, the energy of a digital signal is finite,
but grows with $N$.

%}}}

\section{Power of a signal}
%{{{

By definition, the power $P()$ of a digital (discrete and finite) signal
$\mathbf{x}$ of length $N$ is
\begin{equation}
  P(\mathbf{x}) = \frac{1}{N}E(\mathbf{x}),
\end{equation}
i.e., its average energy.

When working with signals very long ($N$ is big), working with the
power of a signal instead of the energy can be more convenient to
avoid arithmetic overflow.

%}}}

\section{Fourier Transform (FT)}
%{{{

The Fourier Transform of a 1D continuous function $f(x)$ is
\begin{equation}
  F(w) = \int_{-\infty}^{\infty}f(x)e^{-jwx}dx
  \label{eq:FT}
\end{equation}
and its inverse is
\begin{equation}
  f(x) = \int_{-\infty}^{\infty}F(w)e^{jxw}dw,
\end{equation}
where $w=2\pi f$ denotes angular frequency, $j=\sqrt{-1}$, and $f$ frequency.

%}}}

\section{Fourier Transform of a Discrete Function (FTDF)}
%{{{

If the function is discrete (defined only in a set of points in the
domain of the function, for example, time in the case of the sound),
its Fourier transform, also known as the Discrete Time Fourier
Transform (DTFT), is
\begin{equation}
  X(e^{jw}) = \sum_{n=-\infty}^{\infty}x[n]e^{-jwn}
\end{equation}
which is a continuous function of $w$, and its inverse is
\begin{equation}
  x[n] = \frac{1}{2\pi}\int_{-\pi}^{\pi}X(e^{jw})e^{jwn}dw.
\end{equation}
The FTDF is periodic with period $2\pi$.

%}}}

\section{Discrete Fourier Transform (DFT)}
%{{{

The DFT of a digital signal $\mathbf{x}$ (discrete and finite) of
lenght $N$ is defined as
\begin{equation}
  \mathbf{X}_f=\sum_{n=0}^{N-1}\mathbf{x}_ne^{-2\pi jf\frac{n}{N}},\quad f=0,1,\cdots,N-1
  \label{eq:DFT}
\end{equation}
where $f$ denotes (discrete) frequency bins. The inverse DFT is
\begin{equation}
  \mathbf{x}_n=\frac{1}{N}\sum_{f=0}^{N-1}\mathbf{X}_fe^{2\pi jn\frac{f}{N}}, \quad  n=0,1,\cdots,N-1.
\end{equation}

Notice that the DFT can be interpreted as sampling the FTDF at $N$
evenly spaced points over $[-\pi, \pi)$. So, the DFT is a discrete
approximation of the FTDF for finite-length signals.

%}}}

\section{Fourier spectrum}
%{{{

The Fourier transform of a signal $\mathbf{s}$ is a complex signal
$\mathbf{S}$, even if $\mathbf{s}$ is real. By definition, the Fourier
(or frequency) spectrum of $\mathbf{s}$ is the magnitude of
$\mathbf{S}$, that is
\begin{equation}
  |\mathbf{S}| = \sqrt{(\text{Re}(\mathbf{S}))^2+(\text{Im}(\mathbf{S}))^2}.
\end{equation}
As a complex function, we can also find the \emph{phase angle}, \emph{argument} or
\emph{phase spectrum} using
\begin{equation}
  \arg{\mathbf{S}} = \text{arctan}\frac{\text{Re}(\mathbf{S})}{\text{Im}(\mathbf{S})},
\end{equation}
where $\text{arctan}$ must be computed using a four-quadrant
arctangent function.

%}}}

\section{Power spectrum}
%{{{

The power spectrum $P()$ of a digital signal $\mathbf{x}$ shows how
much power a signal has at different frequencies. Therefore, if
$\mathbf{X}$ is the DFT of $\mathbf{x}$ (with $N$ samples),
\begin{equation}
  P(\mathbf{x}) = \frac{1}{N}|X|^2
\end{equation}
is the power spectrum of $\mathbf{x}$.

Notice that, for real-valued digital signals, the spectrum is
symmetric about $N/2$ (Nyquist frequency).

%}}}

\section{Cross-Power Spectral Density}
The Cross Power Spectral Density (CPSD), also known as the
cross-spectrum, analyzes the relationship between two different
signals in the frequency domain. In other words, the CPSD quantifies
the degree to which two signals are correlated or ``statistically
connected'' at specific frequencies. A high CPSD value at a particular
frequency indicates a strong correlation between the two signals at
that frequency.

The $\text{CPSD}$ between two signals $\mathbf{x}$ and $\mathbf{y}$ is
the Fourier transform of the cross-correlation between these two
signals:
\begin{equation}
  \text{CPSD}(\mathbf{x},\mathbf{y})=\mathcal{F}({r(\mathbf{x},\mathbf{y})}).
\end{equation}

Alternatively,
\begin{equation}
  \text{CPSD}(\mathbf{x},\mathbf{y})_f=\mathbb{E}[\mathbf{X}_f\mathbf{Y}_f^*].
\end{equation}
In other words, the CPSD for the frequency bin of index $f$ is the
expectation of the product of the Fourier coefficients $\mathbf{X}_f$
and $\mathbf{Y}_f^*$, for at least two (or more) segments (or
instances) of the signals.

\section{Power Spectral Density (PSD)}
%{{{

The PSD of a digital signal (and in general, of a wide-sense
stationary discrete-time random processes) $\mathbf{x}$,
$\text{PSD}(\mathbf{x})$, is defined as the DFT of its
autocorrelation function $r(\mathbf{x},\mathbf{x})$:
\begin{equation}
  \text{PSD}(\mathbf{x}) = \mathcal{F}(r(\mathbf{x},\mathbf{x})) = \sum_l r(\mathbf{x},\mathbf{x})_le^{-2\pi jfl}.
\end{equation}

Notice also that:
\begin{equation}
  \text{PSD}(\mathbf{x}) = \text{CPSD}(\mathbf{x}),
\end{equation}
and (Wiener–Khinchin Theorem):
\begin{equation}
  r(\mathbf{x},\mathbf{x}) = \mathcal{F}^{-1}(\text{PSD}(\mathbf{x})).
\end{equation}

% }}}

\begin{comment}
\section{Energy Spectral Density (ESD)}
%{{{

The ESD of a signal describes the distribution of the
energy\footnote{That obviously must be finite.} of the signal over
their frequency components. For a discrete-time signal $\mathbf{x}$,
the $\text{ESD}(\mathbf{x})$ is defined as
\begin{equation}
  \text{ESD}(\mathbf{x})=|\mathbf{X}|^2=\mathbf{X}\mathbf{X}^*,
\end{equation}
where $\mathbf{X}$ the DFT of $\mathbf{x}$, and $\mathbf{X}^*$ is its
complex conjugate.

%}}}
\end{comment}

\section{Wiener-Khinching theorem}
\label{sec:WKT}
%{{{

The Wiener-Khinching theorem (also known as the Wiener–Khintchine
theorem or the Wiener–Khinchin–Einstein theorem) states that the power
spectral density (PSD) of a wide-sense stationary random process (its
statistical properties do not change over time) is the Fourier
transform of its autocorrelation function.


states that the
cross-correlation of two wide-sense stationary random processes is the
inverse Fourier transform of the product of the power spectral tensity
of one process and the conjugate of the power spectral density of the
other. Similarly, the autocorrelation is the inverse Fourier
transform of the power spectral density.

% https://engineering.purdue.edu/~bouman/ece637/notes/pdf/WK.pdf
\begin{equation}
  S_x(e^jw)=\sum_kR_x(k)e^{-jwk}
\end{equation}

% }}}


\section{Cross-correlation}
%{{{

The cross-correlation measures the similarity between two signals
$\mathbf{x}$ and $\mathbf{y}$ as a function of the time lag
$l\in\mathcal{N}$ of one relative to the other. In the case of
discrete-time signals, it is defined as
\begin{equation}
  {r(\mathbf{x},\mathbf{y})}_l=\sum_n{\mathbf{x}}_n \mathbf{y}^*_{n-l},
\end{equation}
where $\mathbf{y}^* _{n-l}$ is the complex conjugate of the signal
delayed by $l$ samples. If the signal is real-valued, then
$\mathbf{y}^*_{n-l}=\mathbf{y}_{n-l}$. Notice that
${r(\mathbf{x},\mathbf{y})}$ is a discrete-time signal with so many
samples as diferent $l$ values are used.

Cross-correlation can be normalized using
\begin{equation}
  \rho(\mathbf{x},\mathbf{y})=\frac{{r(\mathbf{x},\mathbf{y})}}{\sqrt{\sum_n \mathbf{x}_n^2 \sum_n \mathbf{y}_n^2}}.
\end{equation}

%}}}

\section{Autocorrelation}
\label{sec:autocorrelation}
%{{{

The autocorrelation of a digital signal is another digital signal that
measures the similarity between the signal and a time-delayed version
of itself as a function of the time lag. Essentially, it quantifies
the degree to which a signal is correlated with its past or future
values. Mathematically, for a discrete-time signal $\mathbf{x}$ of
infinite length, the autocorrelation signal at a lag $l$ is defined
as:
\begin{equation}
  {r(\mathbf{x},\mathbf{x})}_l=\sum_n{\mathbf{x}}_n \mathbf{x}^*_{n-l}
\end{equation}
where $l$ is the integer time lag.

Normalized autocorrelation is defined as
\begin{equation}
  {\rho(\mathbf{x},\mathbf{x})} = \frac{{r(\mathbf{x},\mathbf{x})}}{\sum_n \mathbf{x}_n^2} = \frac{{r(\mathbf{x},\mathbf{x})}}{{r(\mathbf{x},\mathbf{x})}_0}
\end{equation}

Autocorrelation can be computed as the inverse Fourier transform of
the power spectral density.

%}}}

\section{Multivariate data}
Multivariate data refers to datasets that contain more than two
variables for each observation. These variables represent different
characteristics or measurements related to the observed phenomenon. In
simpler terms, if you are collecting information about something and
recording three or more different aspects for each item, you are
dealing with multivariate data. Example: A study on students: You
might collect data on their age, test scores in math, test scores in
science, attendance rate, and extracurricular activities. Each student
is an observation, and the five pieces of information are the multiple
variables.


%}}}

\section{Variance}
The variance of a random variable $\mathbf{x}$, denoted by
$\mathbb{V}(\mathbf{x})$ is a measurement of its dispersion. It is
defined as the expected value of the squared deviation from the mean,
i.e.,
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \mathbb{E}\left[(X - \mathbb{E}[X])^2 \right].
%\end{equation}
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \operatorname{Exp}\left[(X - \operatorname{Exp}[X])^2 \right].
%\end{equation}
\begin{equation}
  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
\end{equation}

\section{Covariance}
\label{sec:covariance}
%{{{

The covariance $\mathbb{V}(\mathbf{x}, \mathbf{y})$ is a measure of
how two random variables $\mathbf{x}$ and $\mathbf{y}$ change
together. In simpler terms, it tells us the direction of the linear
relationship between two variables. The covariance between two
discrete signals $x$ and $y$ is calculated as
%\begin{equation}
%  \text{Cov}(\textbf{x}, \textbf{y}) = \mathbb{E}[(\mathbf{x}-\overline{\mathbf{x}})(\mathbf{y}-\overline{\mathbf{y}})].
%\end{equation}
%\begin{equation}
%  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
%\end{equation}
\begin{equation}
  \mathbb{V}(\textbf{x}, \textbf{y}) = \mathbb{E}\big((\mathbf{x}-\mathbb{E}(\mathbf{x}))(\mathbf{y}-\mathbb{E}(\mathbf{y}))\big).
\end{equation}

We define
\begin{equation}
  \mathbb{V}(\mathbf{x}) = \mathbb{C}(\mathbf{x}, \mathbf{x}).
\end{equation}

%}}}

\section{Covariance matrix}
\label{sec:covariance_matrix}

The covariance matrix $\Sigma_{\overrightarrow{\mathbf{x}}}$ of a random vector $\overrightarrow{\mathbf{x}}=[\mathbf{x}_1,\cdots,\mathbf{x}_N]^T$, defined as,
\begin{equation}
  (\Sigma_{\overrightarrow{\mathbf{x}}})_{i,j}=\mathbb{C}(\mathbf{x}_i,\mathbf{x}_j),
\end{equation}
is a $N\times N$ matrix
\begin{equation}
\Sigma_{\overrightarrow{\mathbf{x}}} = 
\begin{pmatrix}
\mathbb{V}(\mathbf{x}_1) & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_p) \\
\mathbb{C}(\mathbf{x}_2, \mathbf{x}_1) & \mathbb{V}(\mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_2, \mathbf{x}_p) \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{C}(\mathbf{x}_p, \mathbf{x}_1) & \mathbb{C}(\mathbf{x}_p, \mathbf{x}_2) & \cdots & \mathbb{V}(\mathbf{x}_p)
\end{pmatrix}
\end{equation}
that express the covariance between the random variables of a random vector.


\section{$L_2$ norm}
\label{sec:L2_norm}
%{{{

$L_2$ norm is defined by
\begin{equation}
  ||\mathbf{x}||_2 = \sqrt{\sum_i\mathbf{x}_i^2}.
\end{equation}
Notice that the L2 norm and the Mean Square Error (MSE) are closely
related concepts, because
\begin{equation}
  ||\mathbf{x} - \mathbf{y}||_2^2 = J\cdot\text{MSE}(\mathbf{x} - \mathbf{y}).
\end{equation}

%}}}
