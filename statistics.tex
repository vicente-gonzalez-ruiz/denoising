\chapter{Statistics for discrete data}

% We will assume that discrete signals are stationary (discrete) random processes

Random variables

A digital signal is the result of discretizing in amplitude a discrete
signal. Therefore, a sample of a digital signal can be represented
using a finite number of bits.

By physical requirements, digital signals are finite in length, and therefore
$\text{Sup}(\mathbf{x})$ is a finite set.

\section{Discrete signals and random variables}
%{{{

Mathematically, we can model discrete signals as \glspl{SRV}. A
\gls{SRV} can be described as a stochastic process whose statistical
properties do not change with time or space. This means, basically,
that the probability distribution of a signal at any given set of
time/space points is the same as the distribution at those same
samples shifted by any constant amount in time or space.


A digital signal is the results of discretizing a discrete signal in
amplitude. By physical reasons, a digital signal is finite in length
(the number of signal samples is a known number) and in amplitude
(each sample requires a finite number of bits to be
represented). Therefore, digital signals are discrete and finite.

A random variable is a mathematical formalization of a quantity or
object which depends on random events. When more than one value (a
vector) is generated in one of these events, we can establish a
similar connection between multicomponent signals and random
vectors. Statistically, random variables can be described throught the
mean and the variance and the corresponding distribution. In the case
of random vectors, a mean for each component, a covariance matrix, and
a multivariate distribution are required.

In this document, monocomponent (discrete) signals (and therefore
random variables) will be denoted with lower-case bold-faced symbols,
such as $\mathbf{x}$. Multicomponent (discrete) signals (random
vectors) will be denoted as $\overrightarrow{\mathbf{x}}$.

%}}}

\section{Expectation}

\section{Mean}

\section{Variance}
\label{sec:variance}
%{{{

The variance of a random variable $\mathbf{x}$, denoted by
$\mathbb{V}(\mathbf{x})$ is a measurement of its dispersion. It is
defined as the expected value of the squared deviation from the mean,
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \mathbb{E}\left[(X - \mathbb{E}[X])^2 \right].
%\end{equation}
%\begin{equation}
%  \operatorname{Var}(\mathbf{x}) = \operatorname{Exp}\left[(X - \operatorname{Exp}[X])^2 \right].
%\end{equation}
\begin{equation}
  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
  \label{eq:variance}
\end{equation}

%}}}

\section{Covariance}
\label{sec:covariance}
%{{{

The covariance $\mathbb{C}(\mathbf{x}, \mathbf{y})$ is a measure of
how two random variables $\mathbf{x}$ and $\mathbf{y}$ change
together. In simpler terms, it tells us the direction of the linear
relationship between two variables. The covariance between two
discrete signals $\mathbf{x}$ and $\mathbf{y}$ is calculated as
%\begin{equation}
%  \text{Cov}(\textbf{x}, \textbf{y}) = \mathbb{E}[(\mathbf{x}-\overline{\mathbf{x}})(\mathbf{y}-\overline{\mathbf{y}})].
%\end{equation}
%\begin{equation}
%  \mathbb{V}(\mathbf{x}) = \mathbb{E}\left((\mathbf{x} - \mathbb{E}(\mathbf{x}))^2 \right).
%\end{equation}
\begin{equation}
  \mathbb{C}(\textbf{x}, \textbf{y}) = \mathbb{E}\big((\mathbf{x}-\mathbb{E}(\mathbf{x}))(\mathbf{y}-\mathbb{E}(\mathbf{y}))\big).
\end{equation}

Notice that (see Eq.~\ref{eq:variance})
\begin{equation}
  \mathbb{C}(\mathbf{x}, \mathbf{x}) = \mathbb{V}(\mathbf{x}).
\end{equation}

%}}}

\section{Covariance matrix}
\label{sec:covariance_matrix}
%{{{

The covariance matrix $\Sigma_{\overrightarrow{\mathbf{x}}}$ of a random vector $\overrightarrow{\mathbf{x}}=[\mathbf{x}_1,\cdots,\mathbf{x}_N]^T$, defined as,
\begin{equation}
  (\Sigma_{\overrightarrow{\mathbf{x}}})_{i,j}=\mathbb{C}(\mathbf{x}_i,\mathbf{x}_j),
\end{equation}
is a $N\times N$ matrix
\begin{equation}
\Sigma_{\overrightarrow{\mathbf{x}}} = 
\begin{pmatrix}
\mathbb{V}(\mathbf{x}_1) & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_1, \mathbf{x}_p) \\
\mathbb{C}(\mathbf{x}_2, \mathbf{x}_1) & \mathbb{V}(\mathbf{x}_2) & \cdots & \mathbb{C}(\mathbf{x}_2, \mathbf{x}_p) \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{C}(\mathbf{x}_p, \mathbf{x}_1) & \mathbb{C}(\mathbf{x}_p, \mathbf{x}_2) & \cdots & \mathbb{V}(\mathbf{x}_p)
\end{pmatrix}
\end{equation}
that express the covariance between the random variables of a random vector.

%}}}

\section{$L_2$ norm}
\label{sec:L2_norm}
%{{{

$L_2$ norm of a discrete signal $\mathbf{x}$ is defined by
\begin{equation}
  ||\mathbf{x}||_2 = \sqrt{\sum_i\mathbf{x}_i^2}.
\end{equation}
Notice that the L2 norm and the Mean Square Error (MSE) are closely
related concepts, because
\begin{equation}
  ||\mathbf{x} - \mathbf{y}||_2^2 = N\cdot\text{MSE}(\mathbf{x} - \mathbf{y}),
\end{equation}
where $N$ is the length of $\mathbf{x}$.

%}}}

